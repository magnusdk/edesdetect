#+BIBLIOGRAPHY: ../main plain

* Ed-/ES-Detection as a Reinforcement Learning Problem
The goal of this thesis is to explore the potential of using deep RL in the task of ED-/ES-Detection. Thus, the main methods revolve around exploring different formulations of the problem as a RL problem and comparing them.

The standard metric for this task is the Average Absolute Frame Difference (aaFD), as defined in equation [[eqn:aafd]]. aaFD measures the precision and accuracy of predictions by measuring the frame difference between each ground truth event $y_t$ and the corresponding prediction $\hat{y}_t$ generated by the model — a lower aaFD meaning that the model is making fewer errors. $t$ is the index of a specific event, of which there are $N$ in total.

#+NAME: eqn:aafd
\begin{equation}
aaFD=\frac{1}{N}\sum^N_{t=1}|y_t-\hat{y}_t|
\end{equation}

One weakness of aaFD is that it is only defined when there are an equal number of predicted events as there are ground truth events. This is not always the case as an imperfect model may predict more or fewer events. A generalized aaFD ($GaaFD_1$) was considered for a metric instead, calculated as the average frame difference between each predicted event and its nearest ground truth event as in equation [[eqn:aafd_generalized_1]], having the property that it converges towards the true aaFD as the model becomes better. In equation [[eqn:aafd_generalized_1]] $\hat{N}$ is the number of predicted events and $\mathcal{C(y, \hat(y))}$ is the frame difference between the predicted event to the /closest/ ground truth event of the same type. For cases where there are more predicted events than there are ground truth events $GaaFD_1$ would, as is rational, give a worse score. But for cases where there are fewer predicted events than there are ground truth events $GaaFD_1$ would give a score that does not reflect its inability to predict all events.

#+CAPTION: $\mathcal{C}(y, \hat{y}_t)$ is the closest ground truth event from the predicted event $\hat{y}_t$. $\hat{N}$ is the number of predicted events.
#+NAME: eqn:aafd_generalized_1
\begin{equation}
GaaFD_1=\frac{1}{\hat{N}}\sum^{\hat{N}}_{t=1}|\mathcal{C}(y, \hat{y}_t)-\hat{y}_t|
\end{equation}

If we instead calculate the average frame difference between each ground truth event and its nearest predicted event, $GaaFD_2$, as in equation [[eqn:aafd_generalized_2]], we get the opposite problem — too many predicted events are not reflected in the score.

#+CAPTION: $\mathcal{C}(y_t, \hat{y})$ is the closest predicted event from the ground truth event $y_t$.
#+NAME: eqn:aafd_generalized_2
\begin{equation}
GaaFD_2=\frac{1}{N}\sum^N_{t=1}|y_t - \mathcal{C}(y_t, \hat{y})|
\end{equation}

By combining $GaaFD_1$ and $GaaFD_2$ as in equation [[eqn:aafd_generalized]] we mitigate these problems while maintaining the convergence property.

#+NAME: eqn:aafd_generalized
\begin{equation}
GaaFD = \frac{1}{N+\hat{N}}(\sum^N_{t=1}|y_t - \mathcal{C}(y_t, \hat{y})| + \sum^{\hat{N}}_{t=1}|\mathcal{C}(y, \hat{y}_t)-\hat{y}_t|)
\end{equation}

Using GaaFD, or rather its inverse since we want to minimize the error, as a reward function for RL means that we are optimizing the agent directly for our main metric aaFD. It does have one final flaw, however: it is only defined on whole episodes. This means that the agent has to run an entire episode before getting a reward, making the reward signal sparse.

We could instead frame the problem as a simple classification problem where the agent must classify individual frames as either ED, ES, or neither. This allows us to give a reward at each step depending on whether the prediction was correct or not. One problem with this approach is that there is a heavy class imbalance because most frames are neither ED nor ES. A solution to this is to instead predict the phase, either Diastole or Systole, as it is trivial to find ED and ES from the phase by finding the frames where it transitions from one to the other.

From this we can define a simple reward function $R_1$, as seen in equation [[eqn:simple_reward]]. The information that the agent receives from the reward signal $R_1$ is slightly different from the one defined through GaaFD, as GaaFD penalizes predictions that are more wrong heavier than those that are close to the ground truth. We can make the reward signal more similar to GaaFD by defining it in terms of the distance to the nearest predicted phase, as seen in equation [[eqn:proximity_reward]], where $d(s,a)$ is the distance from the current phase $s$ to the nearest predicted phase $a$.

#+NAME: eqn:simple_reward
\begin{equation}
  R_1(s, a) \triangleq
    \left\{
	    \begin{array}{ll}
		    1 & \mbox{if } s=a \\
  	  	0 & \mbox{if } s\neq a
	    \end{array}
    \right\}
\end{equation}

#+NAME: eqn:proximity_reward
\begin{equation}
  R_2(s, a) \triangleq -d(s, a)
\end{equation}

For this thesis, we will be exploring the reward functions $R_1$ and $R_2$.

** Simple Binary Classification Environment
A simple baseline environment is defined as such: The agent, after observing the current and adjacent frames, takes an action predicting that the current frame is either of Diastole or Systole phase, and receives a reward dependent on its prediction before the environment moves the current frame one frame forwards. This environment is visualized in figure [[fig:binary_classification_environment_loop]].

#+CAPTION: Visualization of the Binary Classification Environment loop. An agent sees the observation from the current frame and takes an action, either marking it as Diastole or as Systole, and gets back the reward and the observation for the next frame from the environment.
#+NAME: fig:binary_classification_environment_loop
[[../img/binary_classification_environment_loop.png]]

More formally, the observation $o_t$ at time $t$ is an array of $(2N+1, W, H)$ grayscale pixel values normalized to be between 0 and 1, where $N$ is the number of adjacent frames on either side of the current frame and $W$ and $H$ are width and height, respectively. As a tentative first attempt $N$ is set to 3, making the number of channels of the observations be $3+1+3=7$, and the final shape of the observation be $(7, 112, 112)$, given that we set all videos to be 112-by-112 pixels. The number of adjacent frames is assumed to be an important hyper parameter because it is what gives the agent temporal information.

The agent has a policy $\pi(a|s)$, which, given a state $s$ (which is just the observation in our case), returns probability of taking an action $a$. The goal of the agent is to find the policy $\pi^*$ that, if followed, maximizes our reward function $R$.


** Agent Architecture
The experiments in the next chapter all use a Deep Q-Network (DQN) RL architecture with Prioritized Replay, N-Step returns, and Double Q-Learning. It uses an $\epsilon$ greedy policy to facilitate exploration.

For the Q-network, two architectures are explored:

1. A simple CNN with few layers, inspired by the Atari DQN paper (TODO: citation).
2. MobileNet-v1: a bigger and more complex CNN that uses batch normalization.


** Distributed Training
As mentioned, DQN lends itself nicely to distributed training. In this project, this is achieved through Deepmind's library Acme\cite{hoffman_acme_2020}. At the center of Acme is another library by Deepmind called Reverb\cite{cassirer_reverb_2021}. Reverb is a database for storing experience replay samples that lets us insert and sample experiences independently. If we separate the learning step and the acting step om the algorithm Reverb can be used as the communication point between the two. In this way one or more actors, possibly on different machines, can generate experience samples and insert them into the Reverb experience replay database and a learner, also possibly on a different machine, can sample from it to perform gradient descent. The actors and the learner doesn't need to know about each other, except when an actor needs to update its parameters, in which case it needs to query the learner for the latest trained parameters. It is also trivial to add one or more evaluators that can run in parallel and that only need to query the learner for the latest trained parameters. Inter-process communication is facilitated by a third library, also by Deepmind, called Launchpad\cite{yang_launchpad_2021}.

#+CAPTION: The distributed RL training system. Each pink node runs in a separate Python process, and each blue arrow is a inter-process function call facilitated by Launchpad.
#+NAME: fig:distributed_rl_training
[[../img/distributed_rl_training.png]]

There is a balance to be made between how fast experience samples should be added to the experience replay and how fast they should be sampled by the learner. If the learner samples faster than the actors are able to generate new samples then the network will be trained using trajectories generated from outdated policies. If the actors generate new samples much faster than the learner is able to sample then we are arguably wasting computer resources.

Reverb helps maintain this balance through rate limiters. We use a rate limiter that tries to maintain a specified ratio between insertions and samples, blocking either the actors from inserting new samples or the learner from sampling if the ratio starts to differ too much. Using 6 actors was found to be suffient for generating new experience samples fast enough. TODO: For cases where we use MobileNet also (don't we need less actors there since the learner takes more time)?



TODO: Write about why we chose DQN, what alternatives we considered, etc.

** Discussion
Under the hood, the DQN algorithm is solving a regression problem. Given a state, the model predicts the expected future returns after taking a given action. 




TODO: BCE is using RL for a job that asks for Supervised Learning. There is no exploration, but we still use exploration mechanisms like greedy-epsilon. Using epsilon of 1.0 (100% random decisions while training) is a sign that something is off. It is like an inefficient supervised learning training loop.
- How is this similar to regular supervised learning classification problem?
  - DQN predicts expected future returns of taking an action. We can set up a supervised learning regression problem that predicts the same thing
- We use epsilon=1 and discount=0 — implications?
- Write about how DQN is simply a regression problem
- Future work could be using Policy Gradient methods

* Incorporating Search
RL is a tool meant for solving problems that require search, so in order to get any benefits from it we must transform the problem to one that requires search. This may sound like straightening a screw to make it work with a hammer, and the author sympathizes with this sentiment.

We could let the agent search through frames to find the ED or ES frames. In this case the action set could be to move to the previous, move to the next frame, marking the current frame as ED, or marking it as ES, 4 actions in total. One problem with this formulation is that once the agent has marked one frame as ED or ES, it must know that this state can be ignored and that it should start to look for other ED or ES frames. One work-around to this problem that enforces that the agent visits all frames at least once is simply by initializing it at every frame, but this setup is just a slower, less robus version of the Binary Classification Environment.

#+CAPTION: An agent moves to the previous or next frame and marks frames that it predicts to be ED os ES.
#+NAME: fig:explore_frames_environment
[[../img/explore_frames_environment.png]]

Another option is to perform exploration in space, taking inspiration from papers like (TODO: add paper of RL landmark detection). This can be done by looking at just a small region of interest in the video, which the agent can move around before taking an action. In this way, the agent loses some global context depending on how small the region of interest is, but the smaller input size makes the model less computationally intensive, enabling us to include more temporal information.

#+CAPTION: A Region Of Interest (ROI) is given to the agent which it can then move around in order to explore.
#+NAME: fig:roi_exploration
[[../img/roi_exploration.png]]

Another version of the space-exploration scheme is to let the agent create a synthetic m-mode image from the video. Here, a line can be translated and rotated by the agent, forming the bases of the m-mode image. The pixels along the line for the current frame and some number of previous and next frames are concatenated together into one image. A video can be seen as a 3D data cube, consisting of width, height, and time, but using the synthetic m-mode technique width and height are replaced by the line, effectively removing one spacial dimension while keeping the temporal dimension intact. Compared to the region of interest exploration scheme, synthetic m-mode exploration allows us to keep more temporal data. M-mode imaging is also a well established imaging mode in clinical settings, so this is the method that we want to explore further.

#+CAPTION: An m-mode image is an intersecting plane in 3D "video space".
#+NAME: fig:m_mode_cube
[[../img/m_mode_cube.png]]

** M-Mode Binary Classification Environment
The set of actions remain the same as in the binary classification environment, but an additional 6 actions are added: rotating the m-mode line /clockwise/ and /anti-clockwise/, and translating the line /up/, /down/, /left/, or /right/. The rotation amount and step size are considered hyper parameters. Making them too big would make the line movement less precise, but making it too small would make both the training and inference slower, as well as further sparsifying direct reward signals. A decision also has to be made for whether the translation should be global or local. Global translation means that the line moves in a direction relative to the video, while local translation means that the line moves in a direction relative to where it is pointing towards.

#+CAPTION: Global (to the left) versus local (to the right) translation. Local translation means that the movement depends on the direction of the m-mode line.
#+NAME: fig:local_vs_global_mmode_translation
[[../img/local_vs_global_mmode_translation.png]]

Using local translation is presumed to add some rotational invariance, as the rotation of the video itself can be counteracted by the m-mode line without changing the perceived m-mode effects of translation. This also makes the effects of the up- and down-translations trivial, independent of rotation — it simply shifts the m-mode image down or up, respectively.

TODO: Show how vertical translation (up or down) simply shifts the m-mode image.

At the beginning of an episode the m-mode line is placed in the center of the video, vertically. TODO: This should be at a random position/rotation in the image. The observations returned from the environment is the synthetic m-mode image from the current line, by looking 15 frames in the past and 15 frames in the future, for a total of 31 frames. 8 additional channels are included for the synthetic m-mode images that result from rotating the line to the left and to the right, and moving it to the left and to the right. Synthetic m-mode images resulting from moving the line up or down are not included as channels, as they add very little new information, given that we use a neural network architecture with translational invariance, like CNNs. To counteract the big loss of global context when using synthetic m-mode images, two other channels are included in the observations as well: an average of up to 50 frames around the current frames and an image with the location of the current line drawn onto it. The averaged video image and the current line image adds information about the current line position and provides the agent with some additional context.

TODO: Add image showing m-mode environment and example observations.

The same reward functions are explored as in the regular binary classification environment. In practice, the agent will receive a sparser reward signal, since a reward may only be given when the agent selects to mark the current frame as diastole or systole, not when it only performs translation or rotation.

** Reinforcement Learning Agent Architecture
TODO: Same as before, DQN, but network must be adjusted to fit a tuple of observations (m-mode + overview).

** TODO Discussion
- Sparse reward signal may make the results worse. Can be counteracted by: what? n-step? Less dicsount (gamma closer to 1.0)? Using "advantage" for Q-function? Actor-critic network agent?





* TODO run these experiments (NB: this is not a section in the thesis, just a ToDo-list)
- [ ] Include experiments with different types of sliding windows (with stride, including more frames, etc.)
- [ ] Proximity-based reward should not be $1-distance$, but simply $-distance$. I can't think of a good reason for why it should give 0 on an incorrect prediction that is adjacent to a correct one, instead of -1.
- [ ] Try giving -1 on incorrect predictions for simple reward function instead of 0. What is the result?
- [ ] Ablation study with using "global overview" channels in observation for m-mode env. Does it actually make a difference?














