#+BIBLIOGRAPHY: ../main plain

* Reinforcement Learning in Medical Imaging
RL has seen many medical imaging applications in the last decade, especially in the last five years \cite{zhou_deep_2021}. One of the main challenges of applying RL is formulating the problem to fit into the RL framework of states, actions, and transition and reward function. Out of these four elements, the reward function is usually the most difficult to get right.

One way to formulate the problem is as a search through parameter space. Here, the actions are defined as taking a single step along one of the parameter dimensions. The reward function could be how much closer the agent got to the optimal solution after taking a step (the state and transition function definitions vary depending on the problem). This formulation has been applied to many different medical imaging problems, including that of landmark detection.

The goal of landmark detection is to find a point in an image that represents a medical landmark. In a 2D image, it can thus be defined by the parameters $[x, y]$, where the goal is to find the $x$ and $y$ values that correspond to a given landmark. The state presented to the RL agent will thus be defined in terms of these parameters, such as a smaller section of the image centered around the current point. The action space is defined as a change to the parameters, for example, by increasing or decreasing one of them by some value $\delta$:
\[A = {\pm\delta x, \pm\delta y}\]

The reward signal could be to look at the change of distance to the ground truth landmark after taking an action, which incentivizes the agent to take steps that take it closer to the landmark:
\[R(s_t, s_{t-1}, a) = D(x_{t-1}, y_{t-1}) - D(x_t, y_t)\]

where $D(x, y)$ returns the distance from the point $(x, y)$ to the ground truth landmark. If the distance were $10$ in the previous state and $8$ at the new current state, then the reward would be $10-8=2$. If the distance were $4$ in the previous state and $7$ in the new current state, then the reward, or penalty in this case, would be $4-7=-3$.

This formulation was used for landmark detection in 2D and 3D CT images in a series of papers by Ghesu et al. \cite{ghesu_towards_2018} \cite{ghesu_robust_2017} \cite{ghesu_artificial_2016}. Compared to other state-of-the-art methods at the time, which performed an exhaustive search across the input image, an RL agent only have to follow a simple path, which in the first paper of the series was reported to speed up the detection by 80 times for 2D data and 3100 times for 3D data \cite{ghesu_towards_2018}.

The agent traverses the space by taking a step in one direction, up, down, left, right, forward, and back for 3D images, until it converges around a point that is then considered landmark prediction. Convergence occurs when the agent starts showing oscillating behavior. In the follow-up papers \cite{ghesu_robust_2017} and \cite{ghesu_artificial_2016}, a multi-scale approach was used, wherein the agent searches for the landmark at increasingly fine levels. The first and largest field of view ensures that the agent has access to sufficient global context. When the agent converges, the next scale level is used, and the agent continues searching on this finer scale. A final prediction is made when the agent converges on the finest scale level.

Q-learning is used with a deep CNN as a function approximator, making it a DQN, similar to the model used in \cite{mnih_human-level_2015}. A different model is trained at each scale.

In addition to a strong speed-up and ability to detect landmarks perfectly from the authors’ validation data, the agent can also detect when a landmark is outside of the present scan. In this case, the agent will attempt to leave the image space.

Different versions of DQN and landmark detection problem formulation have been explored. Inspired by the work by Ghesu et al., Alansary et al. explore using a DQN, a Double DQN, a Duel DQN, and a Double Dual DQN for landmark detection in 3D ultrasound and MRI \cite{alansary_evaluating_2019}. The formulation of the problem into state, actions, and reward function remains mostly the same as in \cite{ghesu_robust_2017} and \cite{ghesu_artificial_2016}, except that the state also has a buffer of the last three previously visited states. Including a small history buffer of previous states increases stability and prevents the agent from getting stuck in repeating cycles. Both fixed and multi-scale searching strategies are compared, but the same DQN is shared across all levels in the multi-scale case. They conclude that a multi-scale search strategy improves the performance, especially for large or noisy images, while also speeding up the search process by 4-5 times, but that the choice of deep RL architecture depends on the environment.

A medical image may consist of multiple different landmarks. Vlontzos et al. extend the DQN to a collaborative model where multiple agents share a common CNN but look for different landmarks \cite{vlontzos_multiple_2019}. This is done using a shared CNN, followed by $K$ different sets of fully connected layers, where $K$ equals the number of agents. The fully connected layers learn to find their respective landmarks, while the CNN is trained on data from all the agents at once. This collaborative framework acts as an implicit layer regularization to the network and provides indirect knowledge transfer between agents.

The formulation for treating RL as a search through parameter space has been applied to other tasks as well, such as image registration \cite{liao_artificial_2016} \cite{krebs_robust_2017}, object/lesion localization and detection \cite{maicas_deep_2017}, and more \cite{zhou_deep_2021}.

Image Registration is about aligning two or more images, transforming them into the same coordinate system, and allowing them to provide complementary information in combination. If the transformations can be assumed to be rigid, the set of parameters could consist of simply translation and rotation, making a total of 6 parameters, or 12 actions, for 3D images \cite{liao_artificial_2016}. If the transformations have to be non-rigid, then free form deformations can be used on the image to be registered, such as in the work by Krebs et al. in 2017 \cite{krebs_robust_2017}. In their paper, to reduce the number of actions, they use the first $m$ modes of the PCA as the parameter vector, making a total of $m\times 2$ actions.

Object/lesion localization and detection is the application of object localization to medical imaging. The goal of the algorithm is to find a bounding box around certain objects in the image. For lesion detection in 3D breast scans, Maicas et al. (2017) used a parameter space consisting of translation and scale \cite{maicas_deep_2017}. The agent can take a step along any of the three spatial dimensions or change the scale of the bounding box, making a total of eight actions. Additionally, a ninth action was added that acted as a trigger for when the agent has found a lesion, instead of relying on an agent’s oscillating behavior around the target.

Not all problems fit into this formulation, however. Video summarization is the task of reducing the length of a video while keeping as much useful information as possible. Liu et al. (2020) use RL for summarizing 15 to 65 minutes long fetal ultrasound videos. It is difficult to formulate this problem as a search in parameter space, and therefore the aforementioned reward function based on distance can not be used. Instead, the authors design a reward function that tries to encapsulate what it means to have a good video summarization. The reward function is a sum of three parts:
- $\mathcal{R}_{det}$: the likelihood that a selected frame is of a standard diagnostic plane.
- $\mathcal{R}_{rep}$: the temporal cohesiveness of the selected frames, incentivizing selecting continuous video sections.
- $\mathcal{R}_{div}$: the diversity of the frames, incentivizing selecting frames that are different from each other such that the summarization will be more representative of the whole session.

The action space consists of only two actions: include the current frame or do not include the current frame in the video summary. By using this very simple action-space formulation, and a set of high-level rewards, the agent is still able to achieve good performance. The agent’s predicted summary scores $62.08$ in precision and $64.54$ in recall compared to a user annotated summary.




TODOs:
- [ ] Experiment with also including the m-mode line state from the previous 3 frames, to attempt to decrese chance of getting stuck in loops.










