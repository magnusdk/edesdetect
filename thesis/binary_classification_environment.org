* Introduction
A simple formulation was chosen to act as a baseline. The environment is an ultrasound video of the heart with labels on which cardiac phase each frame is in. For every step, the agent is shown the current frame plus the three previous and the three next frames, and makes a prediction on which cardiac phase the current frame is in. The agent thus have two possible actions: mark the current frame as Diastole or mark it as Systole. After each prediction, the current frame is moved one frame forwards, and the episode ends once the current frame is the last frame of the video.

#+CAPTION: Visualization of the Binary Classification Environment loop. An agent sees the observation from the current frame and takes an action, either marking it as Diastole or as Systole, and gets back the reward and the observation for the next frame from the environment.
#+NAME: fig:bce_loop
[[./img/binary_classification_environment_loop.png]]

The agent doesn't directly predict which frames are ED and which frames are ES, but after an agent has made predictions for every frame we can easily find them by looking at the transitions from predicted Diastolic to predicted Systolic frames.

#+CAPTION: How to find the End-Diastolic (ED) and End-Systolic (ES) from predicted phase. Upper section: good predictions. Lower section: noisy predictions. The noisier the predictions are, the more ED and ES events are incorrectly predicted.
#+NAME: fig:bce_predicted_events
[[./img/ed_es_from_predictions.png]]

Because the job of the agent is to classify each frame as either Diastole or Systole, we name this formulation "Binary Classification Environment".


* TODO Reward functions
The reward function is one of the most important things to consider when designing a RL formulation. It is what guides the agent and must therefore represent our goals for the algorithm. 

The standard for measuring performance in the ED-/​ES-frame detection task is the /average absolute frame difference (aaFD)/. aaFD is defined as
\[aaFD=\frac{1}{N}\sum_{t=1}^N|y_t-\hat{y}_t|\]
where N is the number of ED/ES events, $y_t$ is the ground truth event at time $t$ (of $N$), and $\hat{y}_t$ is the predicted event at time $t$.

aaFD is the key metric that we want to optimize (minimize), and should therefore be considered for the reward function[fn::Of course, since we want to minimize aaFD, the reward should be lower the higher the aaFD is, and vice-versa.]. There are a couple of problems with using aaFD as a reward signal, however: 
1. aaFD is calculated on a whole sequence of frames, meaning that we can't give the agent a reward until it has made a prediction for each frame. This happens at the end of the episode, which is usually tens of frames long in the training dataset. Giving a reward too infrequently makes the agent harder to train and is called the /sparse reward problem/ [TODO: citation?].
2. aaFD is not well defined for when there is an unequal number of predicted events as there are ground truth events, which there invariably will be during training (see lower section of figure [[fig:bce_predicted_events]]).

We could instead continue to treat it as a binary classification problem and give positive rewards for correct predictions, and negative rewards for incorrect ones. Then we would be able to give a reward after every step, making the reward signal no longer sparse.

We must still decide how much reward will be given for correct predictions and how much penalty will be given for incorrect ones. A simple reward function is to give a reward of 1 for correct ones and 0 for incorrect ones:

\begin{equation}
  R_1(s, a) \triangleq
    \left\{
	    \begin{array}{ll}
		    1 & \mbox{if } s=a \\
  	  	0 & \mbox{if } s\neq a
	    \end{array}
    \right
\end{equation}

We could also do the same, but penalize incorrect predictions more:

\begin{equation}
  R_2(s, a) \triangleq
    \left\{
	    \begin{array}{ll}
		    1 & \mbox{if } s=a \\
  	  	-1 & \mbox{if } s\neq a
	    \end{array}
    \right
\end{equation}

We could also penalize incorrect predictions according to how wrong they were:

\begin{equation}
  R_3(s, a) \triangleq 1-d(s, a)
\end{equation}

where $d(s,a)$ is the number of frames to the closest frame with the predicted phase. This gives a reward of 1 for correct predictions, 0 for a one-off incorrect prediction, and otherwise a penalty proportional to how off it was.


TODO: plot reward functions together.




TODO: Note to self:
- Things to try out:
  - Defining a robust aaFD
  - Try to use it as a reward signal at the end of an episode (or balanced accuracy if can't make robust aaFD).
  - Give higher penalties for incorrect predictions that are close to transitions instead of the opposite (as in distance-based).




* TODO Similarities to SOTA, regression (discussion)
Earlier work on detecting ED and ES frames have used a supervised learning approach, formulating the problem as a regression task[TODO: citations from essay]. The line being regressed encodes the phase such that is increasing while in Diastolic phase and decreasing while in Systolic phase. This is in contrast to simply treating the problem as a classification task, using the classes "ED", "ES", and "Trivial", which would result in a class imbalance because the "Trivial" class is greatly over-represented.

Our Binary Classification Environment also gets around the imbalance problem by predicting only the current phase, either Diastole or Systole. There is still a natural class imbalance however, in that there generally are more Diastole frames than Systole, but it is now in a more reasonable range.

Our choice of using DQN as the RL algorithm means that we are also treating the problem as a regression task under the hood. This is because the DQN tries to learn the Q-function, i.e. the expected future return of taking a specific action in a given state. The output of the Q-function applied to each frame in a sequence is two curves, one for each action, and once we have a trained agent agent we select the action with the highest value for each frame. This is the same as in the work by [TODO: cite Lane et al 2021] although they are regressing a single line while we are regressing two, and their ground truth is a hand-crafted curve while ours depends only on the rewards given.

TODO: Include "true" Q-function plots here for different rewards.

For this reason, we don't think that the Binary Classification Environment formulation will perform better than the curent state-of-the-art supervised learning approaches, unless there is some fundamental advantage to regressing on two lines instead of one. It is however a good baseline that we can build upon.


* TODO Evaluation metrics
The standard for measuring performance in the ED-/​ES-frame detection task is the /average absolute frame difference (aaFD)/. aaFD is defined as
\[aaFD=\frac{1}{N}\sum_{t=1}^N|y_t-\hat{y}_t|\]
where N is the number of ED/ES events, $y_t$ is the ground truth event at time $t$ (of $N$), and $\hat{y}_t$ is the predicted event at time $t$.

But as can be seen in figure [[fig:bce_predicted_events]], when the predictions are noisy and imperfect there will be more predicted events than there are in the ground truth. The above definition of aaFD requires there to be an equal number of predicted events as there are in the ground truth. But since the binary classification environment formulation is so similar to just a regular classification problem it lends itself to classification metrics, such as accuracy, precision, recall etc.



* The network, hyper-parameters, algorithm, etc.
Our network is based on the original Atari DQN network. It consists of a convolutional layer with 16 8x8 kernels and a stride of 4, followed by a ReLU activation layer, then another convolutional layer with 32 4x4 kernels and a stride of 2, also followed by a ReLU activation layer. This is then flattened and passed through a fully connected layer with 256 outputs, which is then passed through yet another ReLU layer, before finally being passed through a fully connected with two outputs representing the value of each of the two possible actions.

The agent uses a discount factor of 0.99 and 5-step bootstrapping.



* TODO Choice of frameworks
- JAX
  - Haiku
  - Optax
- Acme
  - Rlax
  - Distributed algorithms with Launchpad
  - Reverb


** JAX
We use JAX[TODO: Cite] for our algorithms, a framework developed by Google. 

** Reverb
An important component of many RL algorithms is the Replay Buffer. In it, the agent stores previous experiences which it can sample from in order to learn about how to better act in the environment in the future. In offline RL there is a natural distinction between acting in the environment and learning from past experiences. The acting process performs actions according to some policy, while the learning process updates the policy with respect to previous experiences. These processes can run independently of each other, as seen in figure [[fig:sep_actor_learner_reverb]]. This allows us the run the algorithm distributedly, with multiple actors generating experience, and possibly multiple learners updating the actors.

#+CAPTION: The separation of actor and learner. Actor and learner are only connected via the replay buffer (except when the actor periodically retrieves the updated policy from the learner).
#+NAME: fig:sep_actor_learner_reverb
[[./img/sep_actor_learner_reverb.png]]

Reverb[TODO: Citation] is a database developed by DeepMind that facilitates training agents distributedly by acting as a replay buffer. There are four main components to a Reverb table:
- The maximum size, which is how many items the table can hold in total.
- The sampler, which determines the strategy for how to sample items from the table.
- The remover, which determines the strategy for how to remove items, for example when the table has reached a maximum size of items.
- And the rate limiter, which can be used to ensure that there is a balance between how often items are inserted and how often they are sampled.

If we want to sample items from the table uniformly (randomly), then we could use a uniform sampler. If we want to remove the oldest items first when the table is full, then we could use a First-In-First-Out (FIFO) remover. If we want to make sure that the actors don't insert items too quickly for the learner, and that the learner doesn't sample items faster than the actors can insert them, we could use a sample-to-insert-ratio rate limiter. The rate limiter will block insertion- or sample-calls to the table until the other component has caught up.

** Acme
Acme[TODO: cite] is another library developed by DeepMind that provides a framework for decomposing an RL agent into multiple building blocks. This, in addition to another library of theirs called Launchpad[TODO: cite], lets us develop an agent that can be run at multiple different scales — i.e. the building blocks can be combined into an agent that runs on a single thread or they can be combined into a distributed agent.



* TODO Performance insight app
- Redo in browser with ClojureScript
- Log metrics for each video over time and show in PerfIn


* TODO Describe dataset (Echonet)



