#+BEGIN_COMMENT
To export to a PDF, run these commands in a scratch buffer:
(add-to-list 'org-latex-classes
  '("ifimaster"
     "\\documentclass[UKenglish]{ifimaster/ifimaster}
      \\usepackage[UKenglish]{ifimaster/uiomasterfp}
      [NO-DEFAULT-PACKAGES]
      [PACKAGES]
      [EXTRA]"
     ("\\section{%s}" . "\\section*{%s}")
     ("\\subsection{%s}" . "\\subsection*{%s}")
     ("\\subsubsection{%s}" . "\\subsubsection*{%s}")))
(setq org-latex-with-hyperref nil)
(setq org-latex-image-default-width "1.0\\linewidth")

...followed by calling the command =org-latex-export-to-pdf=.


After which, to center images, regex-replace:
\\centering\n(\\includegraphics.*\})
\centerline{$1}


#+END_COMMENT

#+BIBLIOGRAPHY: main plain
#+LATEX_CLASS: ifimaster
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc,url}
#+LATEX_HEADER: \urlstyle{sf}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{babel,textcomp,csquotes,graphicx}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{gensymb}
#+LATEX_HEADER: \usepackage[nospace]{varioref}
#+LATEX_HEADER: \usepackage[hidelinks]{hyperref}
#+LATEX_HEADER: \usepackage[backend=biber,style=numeric-comp]{biblatex}
#+LATEX_HEADER: \bibliography{main} 
#+LATEX_HEADER: \DeclareUnicodeCharacter{2212}{-}

#+OPTIONS: toc:nil title:nil author:nil date:nil

\uiomasterfp[
  title=Exploring Reinforcement Learning for End-Diastolic and End-Systolic Frame Detection,
  author=Magnus Dalen Kvalev√•g,
  fac=The Faculty of Mathematics and Natural Sciences,
  dept=Department of Informatics,
]

\frontmatter{}
\chapter*{Abstract}


The thesis explores ways of formulating the problem of detecting the key cardiac phases from ultrasound videos, i.e., the end diastolic (ED) and end systolic (ES) phases, as a reinforcement learning (RL) problem, and whether there are any benefits in doing so. Of particular interest is the design of the RL reward function. Three reward functions are explored: one based on a generalization of the performance metric of average absolute frame difference (aaFD) that is only given to the agent at the end of an episode, and two based on per-frame phase classification given at every step. Additionally, two formulations of the RL environment are explored: binary classification environment (BCE), designed to be a direct reformulation of a supervised binary classification task, and m-mode binary classification environment (MMBCE), designed to provide the agent with the ability to explore the environment using synthetic m-mode imaging. Because of time constraints, MMBCE was only preliminary explored, yet the results indicate that the problem is too complex for the current setup and requires more work before we can draw any conclusions on its feasibility.

Experiments show that an RL agent is able to learn to perform phase detection even when the reward signal is very sparse. However, the less sparse reward functions perform better on nearly all metrics. The best agent predicts the correct number of ED and ES events in $80.26\%$ of the videos on the test set, on which it yields an aaFD score of $1.69$. It is concluded that there are multiple ways of formulating the problem of phase detection as a reinforcement learning problem, but not all formulations are equal. Reward sparsity and environment complexity contribute negatively to performance overall. There are also indications that lower values of the $\epsilon$ -greedy exploration hyperparameter $\epsilon$ have a regularizing effect on the model, prompting further research.









\tableofcontents{}
\listoffigures{}
\listoftables{}

\mainmatter{}



\chapter{Introduction}

* Motivation
Cardiovascular disease is the number one cause of death globally, taking an estimated 17.9 million lives each year \cite{noauthor_cardiovascular_nodate}. It is important to make a timely diagnosis so that patients receive early treatment risk assessment. One standard tool used for diagnosis is cardiac imaging; non-invasive imaging of the heart.

In order to obtain images of the heart, clinicians use tools such as magnetic resonance imaging (MRI), computerized tomography (CT) scans, or ultrasound. MRI and CT are less routinely used due to being expensive, having limited availability and a prolonged acquisition time, and using radiation for CT scans. Furthermore, both MRI and CT scans can not be performed if the patient has any metal in their body, such as a pacemaker or metal implants. Ultrasound, on the other hand, is comparatively inexpensive. It is also more flexible; there even exists handheld devices that can be carried by hand and brought on-site. Ultrasound does have a lower imaging quality compared to, for example, MRI \cite{mordi_efficacy_2017}, and the images can be challenging to interpret due to ultrasound-specific artifacts. Despite this, it is still preferable in many cases because of the reasons mentioned above.

Many heart measurements depend on two key events in the cardiac cycle: end-diastole (ED) and end-systole (ES). Roughly speaking, ED is when the heart is the most relaxed, and ES is when it is the most contracted. Left ventricular ejection fraction is an example of an important measurement that is calculated from ED and ES frames of the cardiac cycle.

A recent study has reported that the average time taken for manually annotating ED and ES frames from visual cues from a video of 1 to 3 heartbeats is 26 seconds, with a standard deviation of $\pm 11$ seconds \cite{lane_multibeat_2021}. Furthermore, because there is not much movement around these frames, the predicted ED and ES frames may differ between different operators. It may even differ for the same operator predicting on the same video at different times. Automating ED/ES frame detection is desirable because it can help reduce annotation time and create a more robust and deterministic result.

Machine learning methods show promising results on several tasks within medical imaging, as is explored in the following chapter. For ED/ES frame detection, the most recent methods utilize supervised deep learning, a family of methods in which a computer program is shown examples of correct predictions and, over time, learns to make the correct predictions itself. Reinforcement learning (RL) is another family of methods that have as of yet not been explored for the problem of ED/ES frame detection. RL is able to outperform humans in complex tasks, such as mastering the board game Go in 2016 \cite{silver_mastering_2016} or becoming among the 0.2% best players in the world in the video game Starcraft II \cite{vinyals_grandmaster_2019}. However, RL can do more than just play games, and many medical imaging applications also show promising potential \cite{zhou_deep_2021}.


* Goal and Research Question
The goal of this thesis is to explore the use of RL for automatically detecting the ED and ES frames from an ultrasound video. It is interesting from a healthcare perspective because it may open the doors for better automated tools. Yet, it is arguably more interesting from a research perspective because RL is not an obvious choice for this task. RL is built for tasks that require strategic reasoning, but ED/ES frame detection is fundamentally a classification problem.

We pose the following research questions:

- *Is it possible to use reinforcement learning for the task of ED-/ES-frame detection?*
- *How does the formulation of the problem as a reinforcement learning problem affect the performance of the model?*

Of importance to all types of machine learning is formulating the problem in a way that makes it easier to learn for the computer. That is, optimizing the /inductive bias/ by incorporating human knowledge into the algorithm itself. Using RL for ED/ES frame detection may open up possibilities of seeing the problem from a new perspective, allowing us the add the right set of inductive bias.




* Limitations of the Work


* Thesis Structure
What are in each chapter...



\chapter{Background}
* The Cardiac Cycle
The human heart is situated in the middle compartment of the chest, between the lungs, and is responsible for keeping the blood flowing by acting as a pump. Blood is used for transporting oxygen and essential nutrients throughout the body and carries metabolic waste such as carbon dioxide to the lungs.

 The heart consists of two halves, the left heart and the right heart, as illustrated in figure [[fig:heart_diagram]]. The left heart pumps newly oxygenated blood from the lungs out to the rest of the body, and the right heart pumps oxygen-depleted blood back to the lungs. Each side has two chambers, the atrium and the ventricle, for a total of four chambers. The upper chambers, the atria, are where the blood first enters the heart, and the lower chambers, the ventricles, are where the blood exits the heart. Each chamber also has valves that are opened and closed during a cardiac cycle to help keep the blood flowing in one direction \cite{iaizzo_handbook_2010}.

 #+CAPTION: An illustration of the heart. The heart has two sides, each side having two chambers. Image reproduced from \cite{noauthor_atrium_2022}, License: CC BY-SA 3.0, User: Eric Pierce (Wapcaplet).
 #+NAME: fig:heart_diagram
 [[./img/heart_diagram.png]]


The stages of the cardiac cycle is illustrated in figure [[fig:cardiac_cycle_heart_illustration]]. During a cardiac cycle, the different chambers are filled at different times. At the start of a new cycle, the left and right ventricles relax and are filled with blood from their respective atria. As the ventricles are filled with blood, the pressure increases, which causes the valves from the atria to close. After this, the ventricles start contracting, pushing blood out from the heart. This causes the ventricle pressure to decrease and the aorta pressure to increase, and the valve going out of the ventricle is closed. Finally, blood flows into the atria before the cycle starts over.

 #+CAPTION: The cardiac cycle is illustrated with the direction of blood flow and pressure from and into the atria and ventricles. Image reproduced from \cite{noauthor_heart_2022}, License: CC BY 3.0, User: OpenStax College.
 #+NAME: fig:cardiac_cycle_heart_illustration
 [[./img/cardiac_cycle_heart_illustration.jpeg]]


There are multiple ways of finding the ED and ES frames in a cardiac cycle \cite{mada_razvan_o_how_2015}:
1. Finding the frame with the maximum left ventricle volume (for ED) and the frame with the minimum left ventricle volume (for ES).
2. Finding the first frame following the closure of the mitral valve (for ED) and the first frame following the closure of the aortic valve (for ES).
3. Analyzing a simultaneously acquired electrocardiogram (ECG) signal.

These methods can be visualized in the Wiggers diagram \cite{mitchell_expanding_2014}, as seen in figure [[fig:wiggers_diagram]], which plots several key events in the cardiac cycle and the corresponding values of various measurements.

Out of these three, using the ECG signal is the least preferable. This is because the methods for detecting the ED and ES frame may become unreliable when given an unconventional ECG signal, such as from patients with cardiomyopathy or regional wall motion abnormalities \cite{mada_razvan_o_how_2015}. Acquiring an ECG signal also requires applying electrodes to the patient, which is not ideal in emergency settings.


 #+CAPTION: The Wiggers diagram describes the different phases of the cardiac cycle and what they represent in different measurements. Image reproduced from \cite{noauthor_wiggers_2021}, License: CC BY-SA 4.0, User: adh30 revised work by DanielChangMD who revised original work of DestinyQx; Redrawn as SVG by xavax.
 #+NAME: fig:wiggers_diagram
 [[./img/wiggers_diagram.png]]

* What is Ultrasound?
In physics, sound can be defined as a phenomenon where energy propagates through a medium ‚Äî such as gases, liquids, or solids ‚Äî by the mean of mechanical waves. In the special case of ultrasound, the waves we refer to are longitudinal pressure waves that are, by definition, slightly above the hearable range of humans (above 20 kHz) \cite{szabo_diagnostic_2014}.

Sound waves push particles together, creating an increase in pressure. Particles in an area of high pressure move to areas of lower pressure, which creates a chain reaction where a pressure field moves through particles. This is called wave propagation and is informally illustrated in figure [[fig:pressure_wave_propagation]]. Sound is simply waves of pressure propagating through a medium.

Today, ultrasound form the basis of several advanced technology such as medical imaging probes, sonar, non destructive testing, and more. This thesis only covers echocardiography, a technology for imaging the heart using ultrasound waves.

#+NAME: fig:pressure_wave_propagation
#+CAPTION: A pressure wave moves through a medium by pushing particles in a medium close together. The particles push back as the pressure increases, moving the pressure field. Warning: This image is just a representation of how particles interact ‚Äî real particles do not look like this.
[[./img/pressure_wave_propagation.png]]




** Attributes of a Sine Wave
A basic wave has three attributes: frequency, how fast it vibrates, amplitude, by how much it vibrates, and phase, where in its cycle a wave is at a given time \cite{manolakis_applied_2011}, as visualized in figure [[fig:amp_freq_phase]]. Our bodies have evolved to sense these properties, where frequency determines the pitch of a sound and amplitude determines the loudness. Sensing phase is a bit more subtle but aid us e.g. in determining the position of the source, relative to us. The relative phase between multiple sounds also affect the resulting sound, as they interfere with each other differently depending on the relative phase.

#+NAME: fig:amp_freq_phase
#+CAPTION: The left-most plot shows two basic waves where one has twice the amplitude. The middle plot shows two basic waves where one has a higher frequency. The right-most plot shows two basic waves that have different phases.
[[./img/amp_freq_phase.png]]

A basic wave means a sine wave in this context. Every sound can be represented as a sum of sine waves, and every sound can be transformed into its frequency spectrum through the use of the Fourier transformation \cite{manolakis_applied_2011}. As seen in figure [[fig:freq_spectrum]], the frequency spectrum of a sine wave is just a single spike. Because of the linear property of the Fourier transform, adding together two sounds has the same effect as adding their frequency spectrums.

Real-world sounds are often more complex than the narrow band sound presented previously. In the nature many acoustic phenomenons can be described by a broadband spectrum, which is a weighted sum of many sine waves. When we hear a piano and a clarinet play the same note, the frequencies with the highest amplitudes are generally the same for both sounds, but the frequency spectrum is much more complex. Musicians speak of overtones ‚Äî it is the overtones that are different for different instruments playing the same notes. They are referring to the additional frequencies that can be seen in the frequency spectrum.

#+NAME: fig:freq_spectrum
#+CAPTION: Adding two sounds together also adds their frequency spectrums together.
[[./img/freq_spectrum.png]]

#+NAME: fig:piano_clarinet_freqs
#+CAPTION: The overtones make two instruments sound different, even when playing the same notes. Left: frequency spectrum of a piano and a clarinet from 150 to 450 hertz. Right: the same frequency spectrum from 0 to 5000 hertz, in log$_{10}$ scale. Both instruments are playing the Am7 chord, which consists of four notes. These four notes can be seen clearly in the left image, all having relatively high amplitudes for both instruments.
[[./img/piano_clarinet_freqs.png]]


** Attributes of the Medium
Another important aspect of sound is the medium through which it travels. Properties such as the speed of sound, density, attenuation, and nonlinearity affect how a sound wave propagates through its medium \cite{johnson_array_1993}. Speed of sound is how fast a wave propagates through the medium. Assuming that the frequency stays the same throughout (which is not always true), the wavelength will be smaller if the sound speed is lower, as visualized in figure [[fig:conveyor_belt_speed_change]]. Density is how tightly packed the particles are in the medium when at rest. Acoustic absorption is an energy loss caused by the viscosity of the propagating medium. The wave energy is then convereted into heat at a molecular level. Attenutation is the reduction of the energy signal caused by either absorption or scattering. Nonlinearity is the property where the speed of sound at a point depends on the pressure at that point. In water, pressure waves propagate faster at higher pressure. The pressure may be caused by the wave itself, in which case the shape of the wave may change, as visualized in figure [[fig:nonlinearity]].

#+NAME: fig:conveyor_belt_speed_change
#+CAPTION: Even though the rate of packages per second stays the same, the distance between packages decreases when arriving on a slower conveyor belt. This is analogous to a sound wave propagating through a medium where the speed of sound changes. Even though the frequency is the same, the wavelength (the length between each top) decreases when it encounters a lower speed of sound.
[[./img/conveyor_belt_speed_change.png]]


#+NAME: fig:nonlinearity
#+CAPTION: In a medium with nonlinearity, higher-pressure parts of a wave propagate faster than lower-pressure parts. Over time, the higher-pressure parts will "catch up" to the lower-pressure parts, and what started as a sine wave will start to resemble a sawtooth wave.
[[./img/nonlinearity.png]]


An important concept is "acoustic impedance," which measures how much resistance the wave encounters while propagating through the medium \cite{szabo_diagnostic_2014}. Acoustic impedance is a function of the speed of sound and density. When a wave propagates out of one medium and into another medium with a different acoustic impedance, a fraction of the energy is reflected. So when one hears a sound being reflected from a wall, it is because the air that the wave travels through and the wall has different acoustic impedance. Equation [[eqn:acoustic_impedance]] shows the relationship between acoustic impedance, density, and speed of sound, where $Z$ is the acoustic impedance, and $\rho$ and $c$ are the density and speed of sound of the medium, respectively. Equation [[eqn:reflection_factor]] is the reflection factor. It determines how much of the energy is reflected, where $Z_1$ is the acoustic impedance of the original medium, and $Z_2$ is the acoustic impedance of the second medium. When $Z_1$ and $Z_2$ are equal, no sound is reflected.

#+NAME: eqn:acoustic_impedance
\begin{equation}
Z=\rho c
\end{equation}

#+NAME: eqn:reflection_factor
\begin{equation} 
RF=\frac{Z_2-Z_1}{Z_2+Z_1}
\end{equation}



* Echocardiography
Light is an electromagnetic signal that does not penetrate very far into the body, which is why we cannot simply gaze into each other's hearts. We could, however, imagine a universe where light penetrates all the way, giving off no reflections at all. In this universe, we would not be able to see the heart either; in fact, we would not be able to see any body at all! To be able to look /inside/ something based on reflections alone requires a sweet spot where the signal can penetrate tissue with enough energy while at the same time being reflected with enough energy so that we can measure it. Arguably, we are quite lucky with our universe, at least in terms of cardiac imaging, because sound is such a signal.

#+CAPTION: Values of the acoustic wave velocity $c$ and acoustic impedance Z of some substances from \cite{suetens_fundamentals_2017}.
#+NAME: tbl:acoustic_impedances
| Substance         | c (m/s) | Z=\rho c (10^6kg/m^2s) |
|-------------------+---------+------------------------|
| Air (25\degree)   |     346 |               0.000410 |
| Fat               |    1450 |                   1.38 |
| Water (25\degree) |    1493 |                   1.48 |
| Soft tissue       |    1530 |                   1.63 |
| Liver             |    1550 |                   1.64 |
| Blood (37\degree) |    1570 |                   1.67 |
| Bone              |    4000 |             3.8 to 7.4 |
| Aluminium         |    6320 |                   17.0 |

Table [[tbl:acoustic_impedances]] lists the speed of sound and acoustic impedance $Z$ of some substances. Notice how there is a large contrast in acoustic impedance between air and soft tissue. If there is air between the sound wave transmitter and the body, most of the energy will be reflected by the skin. To reduce this effect, ultrasound gel, which has a similar acoustic impedance to soft tissue, is applied between the body and the sound wave transmitter. Notice also the difference in acoustic impedance between bone and soft tissue. This has consequences for what we can image in the body, as bones such as the ribcage act as shields to the sound waves.


How can we use sound reflections to create images? We can send out a sound signal and measure the time it takes for a reflection to come back. The delay between sending and receiving gives information about the relative distance to various reflectors in the medium from the sound source, as visualized in figure [[fig:sound_tx_rx]]. Suppose we know the speed of sound, and assume that the speed of sound is homogeneous in the medium. In that case, we can approximate the distance that the wave has traveled by multiplying the delay between sending and receiving by the speed of sound (equation [[eqn:reflector_distance_by_delay]]). This assumes that waves always travel in straight lines, which is not always true, but the effect is often negligible in medical ultrasound use cases.

 #+NAME: eqn:reflector_distance_by_delay
\begin{equation}
\text{distance} = \text{delay} \times c 
\end{equation}

Likewise, suppose we want to know the reflected signal for a given distance away from the transmitter and receiver. In that case, we can calculate the corresponding delay of a signal traveling that distance and back by dividing the total distance by the speed of sound (equation [[eqn:reflector_delay_by_distance]]). When we know the corresponding delay, we can simply look up its value in the signal through interpolation. To create a whole image, we repeat this process for every point in the image.

#+NAME: eqn:reflector_delay_by_distance
\begin{equation}
\text{delay} = \frac{\text{distance}}{c} 
\end{equation}

 #+CAPTION: By measuring the time between sending a signal and receiving it back from a reflector, we can approximate how far away the reflector is ‚Äî given that we know the approximate speed of sound.
 #+NAME: fig:sound_tx_rx
 [[./img/sound_tx_rx.png]]

When we only have a single receiver that measures the reflected sound waves, we can not know the exact location of a given reflector, only the distance. By utilizing more receivers spread over some area, we get more information about where the signal originated from, as there will be a correlation between signals across receivers at the reflecting object.

By utilizing multiple sender elements that can send sound waves independently of each other, we can shape the wavefront as we wish. For example, this lets us focus the energy of the sound wave in a specific area or shape the wavefront to be planar. The Huygens-Fresnel principle states that every point of a wavefront is the source of a new spherical wavefront. We can simulate the Huygens-Fresnel \cite{johnson_array_1993} principle by imagining a desired wavefront passing through the sender elements, activating each element when the wave hits it. Each sender element on its own creates a spherical wavefront, but together they make up the desired imagined wavefront. Time delays are to sound waves like a lens is to a magnifying glass \cite{szabo_diagnostic_2014}. An example of this has been visualized in figure [[fig:huygens_fresnel_focusing]].

#+CAPTION: Because of the Huygens-Fresnel principle, we can create a desired wavefront by creating spherical waves at each sender element when the imagined wavefront hits it. The dashed, pink curve represents the imagined desired wavefront as it approaches the sender elements marked by the purple rectangle. Each sender element is activated when the imagined wavefront passes through it, creating new spherical waves, represented by the cyan semi-circles. The generated spherical waves converge on the same point as the imagined wavefront.
#+NAME: fig:huygens_fresnel_focusing
[[./img/huygens_fresnel_focusing.png]]

In reality, an ultrasound probe consists of many elements acting both as transmitters and receivers. The elements are made out of piezoelectric material. Piezoelectric materials produce vibrations when given an electric current and, vice-versa, produce an electric current when exposed to vibrations. With a transducer, we can independently apply an electric current to each element to create sound waves with given wavefront characteristics and read off the electric current generated by reflected pressure waves \cite{szabo_diagnostic_2014}.



There are multiple modes of ultrasound imaging. The two most important modes for this thesis are B-mode imaging and M-mode imaging \cite{szabo_diagnostic_2014}.

In B-mode (as in "Brightness"-mode) imaging, an image is created by visualizing the amplitude of the reflected signal as the brightness for a given point. This imaging mode often sends out individual, focused transmits in multiple directions, creating a sector scan ‚Äî a fan-like image, as seen in figure [[fig:sector_scan]]. Another method is to transmit unfocused plane waves. A single transmit creates an unfocused image of the scatterers in the medium. However, multiple transmits in different directions may be compounded to create an image of comparable quality to those of focused transmits \cite{montaldo_coherent_2009}.

#+CAPTION: Imaging along different angles from a common starting point creates a sector scan.
#+NAME: fig:sector_scan
[[./img/sector_scan.png]]

B-mode imaging provides images of the whole area of interest, but because they require multiple transmits, they also take longer to acquire, as we have to fire each transmit after the other. In extreme cases, this could pose a problem, given that the heart is an organ that moves quite rapidly. If we are transmitting too slow, then the heart may have a noticeably different phase on one side of the sector scan compared to the other. This is not a significant problem for 2D images as even multiple transmits can be made and received back in a short period of time, but it does have consequences for the temporal resolution.

In M-mode (as in "Motion"-mode) imaging, only one direction is imaged over time instead of a whole sector. This means that it only requires one transmit per frame, giving it a higher temporal resolution compared to B-mode imaging, but at the cost of only focusing in a single direction. Each transmit can be concatenated into an image where the y-axis represents the amplitudes at different depths, and the x-axis represents time, as seen in figure [[fig:m_mode_example]]. M-mode imaging lets us see the motion of a focused part of the heart in a single image.

#+CAPTION: Left: a still of a sector scan. Right: the corresponding M-mode image of the video for the indicated blue line.
#+NAME: fig:m_mode_example
[[./img/m_mode_example.png]]





* Deep Learning
** Gradient Descent
The most significant deep learning innovations have all used a technique called gradient descent.

Gradient descent is based on calculus. It takes advantage of the fact that even if we don't know the true nature of some function, if it is differentiable, then we can calculate its slope at a given point. The slope is also called the gradient and it gives us information about how to update its parameters in order to maximize or minimize the result. This is easily visualized when we have a differentiable function that takes a single parameter $x$, as in figure [[fig:gradient_descent_simple]]. Even though we may now know the true shape of the function, as represented by the dashed line, we can calculate its slope. If we nudge $x$ in the opposite direction of the slope, i.e. reduce $x$ if the slope tends upwards and vice-versa, and repeat this multiple times, then we will eventually reach a minimum where the slope becomes $0$. This iterative process of calculating the gradient at a point and updating the parameters in the opposite direction is what's called gradient descent.

#+NAME: fig:gradient_descent_simple
[[./img/gradient_descent_simple.png]]

Gradient descent scales to an arbitrary number of parameters. This lets us optimize complex models that take a lot of parameters. One example could be that of a model that performs some operation on an image. If we want to process each pixel individually in some parameterizable way then the number of parameters is at least equal to the number of pixels in the image. If the image is 100-by-100 pixels big then the model would take at least $10\,000$ parameters. It is no longer possible to visualize this high-dimensional parameter space as we did in figure [[fig:gradient_descent_simple]], but the principles still hold, and gradient descent still works the same way.



The function that we optimize using SGD consists of two parts: a model and a loss function. The job of the model is to perform the task at hand, and the job of the loss function is to quantify the error of the model so that we can minimize it. As long as both the model and the loss function is differentiable then we can optimize it using SGD. Not all models and not all loss functions are equally good, however. Some models may better represent the problem at hand than others and some loss functions may produce gradients that are easier to optimize for than others. One important aspect is the shape of the gradient and whether it contains a lot of local minima. 


We may want to optimize some parameters working on a set of images, for example when training a model to classify pictures as those of cats or of dogs. We may consider the data set as part of the loss function, as we want to minimize the error of the model on these specific data. Because of either memory or computational constraints, there may be too many pictures in the dataset for the model to try to optimize for at once. In this case it is common to apply gradient descent on just a subset of the full dataset at once, chosen randomly at each iteration. This is called stochastic gradient descent (SGD) and, perhaps surprisingly, it is often better at generalizing on the dataset than using gradient descent on the whole dataset at once.

Another aspect of great importance is to instill what's called inductive bias into the model; that is, implicit knowledge about the task at hand. Some models capture implicit knowledge about the problem at hand better than others, and some important features are explored in the next section.




** Deep Neural Networks
Fully connected layers is just a matrix multiplication between the input $x$ and a weigth matrix $w$, often with an additional bias $b$ added, such that $\hat{y} = wx+b$. The shape of the matrix $w$ determines the number of output neurons it produces.

Multiple fully connected layers can be stacked to create a more complex network. However, beneath the hood each layer is performing matrix operations, which are linear operations, and no matter how linear layers are stacked they can only represent linear relationships. To allow the network to represent more complex, non-linear relationships we need to additionally add non-linearity to the network. This is usually done using activation functions. Examples of activation functions are the sigmoid function, seen in equation [[eqn:sigmoid_activation]], or ReLU, seen in [[eqn:relu_activation]].

#+NAME: eqn:sigmoid_activation
\begin{equation}
S(X) = \frac{1}{1+e^{-x}}
\end{equation}

#+NAME: eqn:relu_activation
\begin{equation}
ReLU = max(0, x)
\end{equation}

Using just two fully connected layers separated by non-linear activation functions one could represent any arbitrary function, given that one includes enough neurons [TODO: Cite "Multilayer Feedforward Networks are Universal Approximators"]. That does mean that they are the right tool for every job.

Fully connected layers combine every input with every output. This does not take advantage of the spatial locality of images. In an image a given pixel is often more related to pixel who lie closer to it. Convolutional layers take advantage of this by applying filters to an image, with each filter only processing a small part of the image at a time. The filters are often small matrices that is applied everywhere in an image.

Each filter has a width and height which determine how many pixels are included in one application of the filter. One may also affect how the filters are applied to an image through stride, which determine how much the sliding window of the filter "skips" for each application. For example a stride of 2 means that the filter moves two pixels for each application. Another hyper-parameter is dilation, which affect the spacing between pixels for an individual application. A dilation of 2 means that every other pixel is ignored when applying the filter.

Another popular layer is the recurrent layer. The recurrent layer is a way of processing sequential data, such as temporal data. Recurrent layers have a unit with hidden state that is applied at every time step. One popular version of a recurrent layer is LSTM [TODO: citation].


** Optimization Process
For each iteration of gradient descent we update the model's parameters a small step in the opposite direction in order to minimize it. It is important to only update them in a small step each time, otherwise they may overshoot and in the worst case cause the model's performance to diverge. For standard SGD we choose how much to update the parameters using the hyper-parameter $\alpha$ which is often a low number between 0 and 1.

Using a low $\alpha$ means that we don't update the parameters too much when the gradient is steep, but it also means that the parameters are updated very little when the gradient is near-flat. In addition, we may encounter flat regions of the gradient which can be hard to move past regardless of the chosen value of $\alpha$. For these reasons other optimizers have been developed, such as ADAM [TODO: cite].

TODO: Explain ADAM




** Overfitting and Regularization
The goal of machine learning is to train models that generalize to data samples outside of the training set. When we optimize a model on a given data distribution we risk making the model specialize too much on that specific distribution. When the model performs significantly better on the data it has been trained on versus unseen data we say that it has overfit.

One way to reduce the chance of overfitting is to use regularizers. Regularizers either augment the training data or put additional constraints on the optimization process such that the the model is likely to overfit. One example is to use data augmentations: random transformations on the training data. In practice, this increases the training dataset as more data are added to it. One could also augment the loss function itself by adding the magnitude of all the parameters, thereby encouraging the network to be less dependent on a small number of features.



** Supervised, Semi-Supervised, and Unsupervised Learning
One way of designing the loss function is to define it as the difference between the predicted values from the model and ground truths that were labeled beforehand. Learning methods that use these kinds of loss functions are generally called supervised learning, as if a "supervisor" tells the model what the right answer ought to have been.

If we don't have access to ground truth labels we can instead define the loss in other ways. TODO: semi-supervised and unsupervised





TODO: Run Grammarly on above sections (once they are finished).


** Reinforcement Learning
RL allows an agent to learn a strategy, called a /policy/, that maximizes the total reward received through interacting with an environment. RL can leverage time in a way that neither supervised nor unsupervised learning is able to because it takes future decisions into account when deciding on the next action. An RL agent can make a decision now that has no immediate benefit but will lead to a better result in the future.

 At the core of RL are markov decision processes (MDP) \cite{sutton_reinforcement_2018}, which can be described using four elements:

 - The state space $S$
 - The action space $A$
 - The transition function $P(s_{t+1}|s_t, a_t)$
 - The reward function $R(s_t, a_t)$

 An RL agent is faced with a sequence of decisions. At each step, it is presented with the current state $s_t \in S$ of the environment and must take an action $a_t \in A$. In an episodic task, the agent's goal is to maximize the total reward $r$ it receives during its lifetime, called an episode. The environment may change after the agent takes an action in a given state, and how it changes, i.e., what the next state $s_{t+1}$ will be, is determined by the transition function $P(s_{t+1}|s_t, a_t)$. How much reward the agent receives after taking an action in a given state is determined by the reward function $R(s_t, a_t)$. The goal of RL is to find a policy $\pi$, a strategy that, if followed, will yield the most amount of total reward during the lifetime of the agent. In practice, the policy is simply a function that takes in the current state $s_t$ and returns the probability of taking an action $a_t$: $\pi(a|s)\in[0,1]$.

 The agent's goal is not to maximize the immediate reward $r$ but rather the expected return. The return is denoted as $G_t$ and is in its simplest form a sum of all the future rewards, as seen in equation [[eqn:returns]]. $T$ marks the timestep where the episode ends.

#+NAME: eqn:returns
\begin{equation}
G_t = r_{t+1} + r_{t+1} + r_{t+2} + \ldots + r_T
\end{equation}


However, some tasks are not episodic, which means that they may run forever. The returns $G$ becomes infinite for environments with limitless rewards, making the optimization problem intractable. To solve this problem we include /discounting/ to the returns, as seen in equation [[eqn:discounted_returns]]. $\gamma$ is the /discount rate/ and is a number in the range $[0,1]$. If $\gamma<1$, then future rewards count for less in the full returns, and as the number of steps into the future approaches infinity, the corresponding rewards approach $0$. Discounting guarantees that non-episodic tasks converge to optimal solutions while also giving a mechanism for preferring more immediate rewards compared to future rewards.

#+NAME: eqn:discounted_returns
\begin{equation}
\begin{aligned}
G_t &= r_{t} + \gamma r_{t+1} + \gamma^2 r_{t+1} + \ldots \\
    &= \sum_{k=0}^\infty \gamma^k r_{t+k+1} \\
    &= r_{t} + \gamma G_{t+1}
\end{aligned}
\end{equation}

One way to select an action is to predict the following state's value after taking that action. For this we could use the /state value function/ $V_\pi(S_t)$ which estimates the expected return $G_t$ of being in state $s_t$, while following the policy $\pi$. Alternatively, we could use the /state-action value function/ $Q_\pi(s_t, a_t)$ which estimates the expected return of taking action $a_t$ in state $s_t$, while following the policy $\pi$. Both value functions depend on the policy being followed because the policy decides what actions to take in the future, which again has consequences for what rewards the agent expects to receive at subsequent steps. For this setup, the "learning" part of RL could be considered to be updating a value function towards the "optimal value function," defined as the value function that uses the optimal policy when estimating returns. The optimal policy $\pi^*$ is one /(of the possibly many policies)/ that yields the maximum amount of total reward if followed.

TODO: Mention epsilon-greedy policy and UCB

One algorithm for updating the state value function is called temporal difference learning (TD). In TD, the state value function $V(s_t)$ is updated after every step, by comparing the value it expected to see, with a value that takes the newly observed reward $r_{t+1}$ into consideration, as seen in equation [[eqn:state_value_function]]. $(r_{t+1} + \gamma V(s_{t+1}))$ is called the TD-target, and because it incorporates the actual observed reward $r_{t+1}$, it can be considered as a more up-to-date version of the state value function. $(r_{t+1} + \gamma V(s_{t+1})) - V(s_t)$ is called the TD-error. The lower the TD-error is, the better the RL agent is able to reason the value of states, and as such, we want to minimize it. We do this by updating the state value by nudging it slightly towards the TD-target. How far it is nudged at each update is determined by $\alpha$.


#+NAME: eqn:state_value_function
\begin{equation}
V(s_t) \leftarrow V(s_t) + \alpha[(r_{t+1} + \gamma V(s_{t+1}))-V(s_t)]
\end{equation}


 To be able to use $V(s)$ for making a decision, the agent needs knowledge about the transition function $P(s_{t+1}|s_t, a_t)$. This is because it needs to know what the next state will be to select the best action to take. $Q(s, a)$ does not need knowledge about the transition function because it directly learns the value of taking an action for a given state. TD can be modified to use the state-action value function instead of the state value function, in which case it is called Q-learning. In equation [[eqn:state_action_value_function]], the target (Q-target), is defined as the immediate reward of taking action $a_t$, plus the discounted value of taking the best action in the following state.

#+NAME: eqn:state_action_value_function
\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [(r_{t+1} + \gamma max_a Q(s_{t+1},a))-Q(s_t, a_t)]
\end{equation}


 In TD-learning, the agent must associate each state with its corresponding value as it explores the environment. The same is true for Q-learning, but it also has to take state-action pairs into account, meaning that it has to store up to $\|S\| \times \|A\|$ entries. That is fine when the state- and action-space are small but becomes infeasible when they are too big.

 The described way of storing and updating the values is called tabular methods because we treat the states, or state-action pairs, as entries in a table. Tabular methods break down when the state space or the action space becomes very large or even continuous. Creating RL algorithms that can handle very large or continuous action spaces is challenging \cite{zhou_deep_2021}. However, methods exist that can scale RL to handle very large or continuous state spaces.


*** Deep Reinforcement Learning
 A modified Q-learning algorithm has been shown to be able to play Atari games simply by looking at the raw pixel values \cite{mnih_human-level_2015}. The state-space thus consists of the pixel values of the current game screen. A simple Atari game has $210\times 160 = 33600$ pixels, and each pixel can be one of $128$ colors \cite{mnih_human-level_2015}. In theory there are $128^{33600} \approx 10^{70803}$ different states. If a computer were able to process $1\,000\,000\,000$ such states every second, it would still take more than $10^{70785}$ years to process all of them.

We assume that there exists a way to approximate the value of states in a much more compressed way. This can be done through function approximation \cite{sutton_reinforcement_2018}, where instead of storing and updating the value estimates in a table, such as with tabular methods, they are approximated using a neural network. This may also allow the agent to generalize state value or state-action value functions to new not-before-seen states.

 Much of today's research into RL goes into scaling it up to larger state-spaces. Methods that scale RL by modifying the Q-learning algorithm are called "action-value methods," but they are not the only ones to do so. Policy gradient is another popular set of methods that can learn a parameterized policy directly, without consulting a value function \cite{sutton_reinforcement_2018}. Policy gradient methods may more naturally model continuous action spaces as it outputs a distribution of action probabilities instead of the values of a discrete set of actions. As seen in later chapters, the RL formulations used in this thesis all use discrete action spaces, and only action-value methods are considered for this thesis.

*** Deep Q-Network
The modified Q-learning algorithm was termed deep q-network \cite{mnih_human-level_2015} (DQN) for its ability to take advantage of recent deep learning advances and deep neural networks.

The original DQN algorithm takes the raw pixel values from an Atari game as input, followed by three convolutional layers and two fully connected layers. The final fully connected layer outputs one value for each possible action, approximating the expected value of taking each action given the state, i.e., $Q(s, a)$. An $\epsilon$ -greedy policy then chooses either the action with the highest approximated value with probability $1 - \epsilon$ or a random action with probability $\epsilon$.

The authors showed how the network is able to reduce the state space by applying a technique called "t-SNE" to the DQNs' internal state representation. t-SNE is an unsupervised learning algorithm that maps high-dimensional data to points in a 2D or 3D map \cite{liao_artificial_2016}. As expected, the t-SNE algorithm tends to map the DQN representation of perceptually similar states to nearby points. Interestingly, it also maps representations that are perceptually dissimilar, yet are close in terms of expected rewards, to nearby points. This indicates that the network is able to learn a higher-level, but lower-dimensional, representation of the states in terms of expected reward. This is visualized in figure [[fig:dqn_atari_t_sne]].


#+CAPTION: A figure from \cite{mnih_human-level_2015} that shows a two-dimensional t-SNE embedding of the representations in the last hidden layer assigned by DQN to game states experienced while playing Space Invaders. The points are colored according to the state values predicted by DQN for the corresponding game states. The states rendered in the top right, which are of almost full of enemy ships, and the states rendered in the bottom left, which are nearly empty, have similar predicted state values even though they are visually dissimilar, because the agent has learned that completing a screen leads to a new screen full of enemy ships. 
#+NAME: fig:dqn_atari_t_sne
[[./img/rl_dqn_tsne.jpeg]]

Using function approximation does have its problems. Naively training the network by inputting state and returns pairs as the agent generates them can make the algorithm unstable. There is a strong correlation between consecutive samples, and if a neural network receives a batch of very similar input, it might overwrite previously learned knowledge. Furthermore, an update that increases $Q(s, a)$ often also increases $Q(s+1, a)$ and therefore also increases the target value, possibly leading to oscillations or divergence of the policy. These problems are mitigated by using experience replay and by using a separate network to generate the targets in the Q-learning update.

In experience replay, the agent's experiences over multiple episodes are stored in a data set called the replay memory. Each experience item is a tuple consisting of the previous state, selected action, returned reward, and new state: $(s_t, a_t, r_t, s_{t+1})$. During training, randomly sampled batches from the replay memory are used to train the Q-network.

Using a separate network for generating the targets in the Q-learning update adds a delay between the time an update to Q is made and the time it affects the targets, making the algorithm more stable and reducing the chance of oscillations or divergence.




*** Double Deep Q-Network
Several improvements have been made to DQN over the years. Q-learning has been shown to produce overly optimistic action values as a result of using the maximum action value as an approximation for the maximum expected action value \cite{h_p_van_hasselt_hado_double_2010}. Double Q-learning attempts to reduce this overestimation by decomposing the target into an action selector and an action value estimator. The regular Q-learning target is written as:

\[r_{t+1} + \gamma max_a Q(s_{t+1},a)\]

This can be rewritten as:

\begin{equation}
r_{t+1} + \gamma Q^A(s_{t+1},argmax_a Q^B(s_{t+1},a))
\end{equation}

Where $Q^A$ acts as an action value estimator and $Q^B$ acts as an action selector. If $Q^A=Q^B$, then this is just the regular Q-learning target. If we only update the action selector at each update and randomly choose which of the two Q-functions should be used as the action selector at each update, the overestimation is reduced. This also applies to DQN, and it has been shown that using a double DQN results in better policies than using a regular DQN \cite{van_hasselt_deep_2015}.

*** Prioritized Replay
By using experience replay, agents are not forced to process transitions in the exact order that they are experienced. However, because we are sampling the transitions uniformly from the replay memory, all transitions are given equal priority. We might benefit from prioritizing transitions that have a high TD-error magnitude, which acts as a proxy measure of how "surprising" a transition is to the agent \cite{schaul_prioritized_2016}.

Prioritizing experience by the magnitude of the TD-error may introduce a lack of diversity. One of the reasons for this is that an experience that initially had a low TD-error, but that later becomes large as the network is trained, will continue to be down prioritized because the TD-error is only updated when the transition is revisited ‚Äî and because of its low prioritization, the probability that it will be revisited soon is low. A stochastic sampling method that interpolates between pure greedy prioritization and uniform random sampling is introduced to overcome this challenge.

Another problem with prioritized experience replay is that DQN minimizes the expected TD-error squared with respect to the network parameters $\theta$, assuming that the samples in the replay buffer correspond to the same distribution as seen while exploring. Prioritized experience replay breaks this assumption, introducing a bias in the calculated gradient. This is fixed by using importance sampling, such that the less-sampled experiences are compensated for in the gradient. As the unbiased nature of the updates is most important near convergence at the end of the training, the importance sampling is gradually added towards the end, with less importance sampling included at the start of training.

Prioritized replay is found to speed up an agent's ability to learn by a factor of 2.

*** Dual Deep Q-Network
In the dueling architecture, or Dual DQN, the network that approximates the Q-function is split into two parts: one for estimating the value of the current state and one for measuring the so-called advantage of taking an action in this state \cite{wang_dueling_2016}. The combination of the state-value estimate and the advantage yields the Q values:

\begin{equation}
Q(s,a)=V(s)+A(s,a)
\end{equation}

However, because the state value function $V(s)$ can be expressed in terms of the state-action value function $Q(s,a)$ by taking the mean of $Q(s,a)$ over all actions, then it means that the mean of the advantage function $A(s,a)$ over all actions equals zero. This is not necessarily the case because the networks are simply approximations. To fix this issue, the authors also subtract the mean advantage from the equation. This change loses the original semantics of $V(s)$ and $A(s,a)$ but results in a more stable algorithm.

\begin{equation}
Q(s,a)=V(s)+A(s,a)-\frac{\sum_{a}A(s,a)}{N_{actions}}
\end{equation}

The dueling architecture lets the network train the state-value and advantage functions separately.

*** Multi-Step Learning
We look only one step ahead when constructing the target in the Q-learning update, but this is not a requirement. We could extend it to look $N$ steps ahead if we wanted to, which is called N-step learning or multi-step learning \cite{sutton_reinforcement_2018}.

To use multi-step learning we must look at $N$ consecutive experiences for every update, and sum the appropriately discounted rewards and add it to an appropriately discounted value estimation of the final state in the sequence. The N-step target for a given state $s_t$ is given as:

\begin{equation}
\sum_{k=0}^{N-1}\gamma^k r_{t+k+1} + \gamma^N max_a(Q(s_{t+N}, a))
\end{equation}

If we set $N$ to be 1, the algorithm would equal the standard Q-learning algorithm. As we increase $N$, the algorithm would become more and more similar to the Monte Carlo method, which looks ahead all the way until the agent hits a terminal state.


\begin{equation}
r_{t+1} + \gamma max_a Q(s_{t+1},a)
= \sum_{k=0}^{n-1}\gamma^k r_{t+k+1} + \gamma^n max_a(Q(s_{t+n}, a))\textrm{, iff n=1}
\end{equation}

The best choice of $N$ usually lies somewhere between 1 and the length of an episode. This is because bootstrapping works best when it is over a length of time in which a significant and recognizable state change has occurred. Another intuition for why multi-step learning improves performance is that when we look further ahead, we depend less on our estimates of the future.

*** Distributional Reinforcement Learning
The Q-function is an approximation of the /expected/ returns, but it is also possible to approximate the /distribution/ of returns instead \cite{bellemare_distributional_2017}. It makes sense to think about the returns as a distribution, even when the environment has deterministic rewards, because stochasticity is still introduced while training through various sources. Firstly, state aliasing, the conflation of two or more states into one representation, may cause different amounts of rewards to be observed even though the agent "sees" the same state. Secondly, because of bootstrapping, target values are nonstationary while training, and the returns will take on different values over time. Lastly, approximation errors will make the returns seem stochastic because we only approximate the true Q-function.

Approximating the distribution of returns instead of the expected returns results in more stable learning targets.

*** Noisy Deep Q-Network
Exploration of the environment is often enabled by using an $\epsilon$ -greedy policy, where $\epsilon$ is gradually reduced. For particularly hard problems, like the Atari game "Montezuma's Revenge", this technique becomes insufficient for exploration \cite{bellemare_unifying_2016}. $\epsilon$ -greedy policies explore with a fixed probability that is the same for every state. An alternative could be to let the network itself learn when it should explore, and for what states. 

NoisyNet-DQN does this by applying learnable parameterized noise to the value network parameters \cite{fortunato_noisy_2019}. This does not only enable it to change the amount of exploration itself, alleviating the need for hyperparameter tuning, but also to apply different amounts of exploration to different states.

*** Rainbow Deep Q-Network
Many of the improvements that has been made to DQN may be complementary and could be combined into a single algorithm. The Rainbow \cite{hessel_rainbow_2017} algorithm combines six such improvements:

1. Double DQN \cite{van_hasselt_deep_2015}
2. Prioritized replay \cite{schaul_prioritized_2016}
3. Dual DQN \cite{wang_dueling_2016}
4. Multi-step learning \cite{sutton_reinforcement_2018}
5. Distributional RL \cite{bellemare_distributional_2017}
6. Noisy DQN \cite{fortunato_noisy_2019}

The authors show that the combined algorithm performs much better than each extension alone in terms of both learning speed and overall performance.

They also performed an ablation study on the Rainbow algorithm to see how much each extension contributes to its overall performance. The study concludes that prioritized replay and multi-step learning contribute the most to the overall performance, as removing them from the algorithm reduces its performance the most. Distributional Q-learning ranked directly below, followed by Noisy DQN, and then Dual DQN. The benefit of using a Double DQN is not apparent, as removing it from the algorithm does not reduce its performance.

#+CAPTION: A figure from \cite{hessel_rainbow_2017} showing the median performance of multiple modified DQN agents compared to human performance across 57 Atari games. After 200 million frames, all modifications show an improvement over regular DQN, but together (Rainbow), they perform significantly better than any one single improvement. Curves are smoothed with a moving average of 5 points.
#+NAME: fig:dqn_rainbow_parts_perf
[[./img/dqn_rainbow_parts_perf.png]]


#+CAPTION: A figure from \cite{hessel_rainbow_2017} visualizing an ablation study of the various DQN modifications (dashed lines). Dashed lines that are close to the rainbow line indicate that the corresponding DQN modification does not add much benefit to the overall agent or is overshadowed by other modifications. According to the ablation study, the three most important modifications are N-step bootstrapping (multi-step), distributional Q-learning, and prioritized replay.
#+NAME: fig:dqn_rainbow_ablation
[[./img/dqn_rainbow_ablation.png]]










* Related Work (State-of-the-art Section (TBD))
** ED-/ES-Detection
#+INCLUDE: parts/previous_work.org

** Reinforcement Learning in Medical Imaging
#+INCLUDE: parts/rl_in_medical_imaging.org


TODO: Grammarly on the previous 2 sections


This work serves as inspiration for this thesis and helps guide our RL formulations for the task of ED-/ES-frame detection.











\chapter{The Dataset}
Overview of the chapter.
Short description of the different datasets used.


* Echonet-Dynamic Dataset
The Echonet-Dynamic Dataset \cite{ouyang_echonet-dynamic_2019} is an openly available collection of 10,030, 112-by-112 pixels echocardiography videos for studying cardiac motion and chamber volumes. Each video has been cropped and masked to exclude text, ECG- and respirometer-information, and downsampled from its original size into 112-by-112 pixels using cubic interpolation. All videos are of the apical-4-chamber view, and each video is from unique individuals who underwent imaging between 2016 and 2018 as part of routine clinical care at Stanford University Hospital. Images were acquired by skilled sonographers using iE33, Sonos, Acuson SC2000, Epiq 5G, or Epiq 7C ultrasound machines. Each video has been labeled by a registered sonographer and verified by a level 3 echocardiographer in the standard clinical workflow.

The dataset consists of three parts: /FileList.csv/ contains general information about each video, its variables are listed in table [[tbl:echonet_filelist_variables]]. /VolumeTracings.csv/ contains the volume tracings and ED/ES frame index of each video, its variables are listed in table [[tbl:echonet_volumetracings_variables]]. And finally /Videos/, containing all the ultrasound videos in =.avi= format. Video frame samples can be seen in figure [[fig:echonet_samples]].

#+CAPTION: Echonet video general information variables.
#+NAME: tbl:echonet_filelist_variables
| Variable       | Description                                                        |
|----------------+--------------------------------------------------------------------|
| FileName       | Hashed file name used to link videos, labels, and annotations      |
| EF             | Ejection fraction calculated by the ratio of ESV and EDV               |
| ESV            | End systolic volume calculated by the method of discs                  |
| EDV            | End diastolic volume calculated by the method of discs                 |
| FrameHeight    | Video Height                                                       |
| FrameWidth     | Video Width                                                        |
| FPS            | Frames Per Second                                                  |
| NumberOfFrames | Number of Frames in the whole video                                    |
| Split          | Classification of train/validation/test sets used for benchmarking |

#+CAPTION: Echonet video volume tracing variables
#+NAME: tbl:echonet_volumetracings_variables
| Variable | Description                                                   |
|----------+---------------------------------------------------------------|
| FileName | Hashed file name used to link videos, labels, and annotations |
| X1       | X coordinate of the left-most point of line segment               |
| Y1       | Y coordinate of the left-most point of line segment               |
| X2       | X coordinate of the right-most point of line segment              |
| Y2       | Y coordinate of the right-most point of line segment              |
| Frame    | Frame number of video on which tracing was performed          |


#+CAPTION: The first frames of 15 randomly sampled videos from the Echonet dataset.
#+NAME: fig:echonet_samples
[[./img/echonet_samples.png]]

** Getting ED/ES Frame Information
To get the ED and ES frames, we have to look at the volume tracings, whose variables are listed in table [[tbl:echonet_volumetracings_variables]]. The volume tracings list the line segments that define the heart's volume at a given frame. There are two sets of line segments for each video, one for ED and one for ES, but which one is which is not given explicitly. We can find this information by calculating the volume from the line segments for both frames and comparing them ‚Äî the one with the largest volume is ED, and the other one is ES.

** Extrapolating Diastole and Systole Labels
As is explored in later chapters, we would also like to label the phase of each frame in the video, not just the frame that ends each phase. When we only have access to the end-frames of each phase, the first phase will only have one labeled frame. For example, if the ED frame comes first, then only the first frame will be labeled diastole as the rest will be systole, as visualized in figure [[fig:echonet_label_imbalance]].

#+CAPTION: Class imbalance: only the first frame is marked with the phase of the first end-event (either ED or ES). All others are marked with the other phase.
#+NAME: fig:echonet_label_imbalance
[[./img/echonet_label_imbalance.png]]

We can extract more frames before and after the labeled frames by exploiting the periodicity of the cardiac cycle. As the heart goes from one phase-end to the other, the difference between the current frame and the first phase-end differs more and more. When the opposite end-phase is reached, the frames will start to differ less. For example, the next frame with the biggest difference from the ED frame is likely close to the ES frame. This periodic effect can be seen if we plot the absolute difference between a frame and the rest of the video, as seen in figure [[fig:frame_difference_plot]].

#+CAPTION: The absolute frame difference of all frames in a video compared to frame 100. Notice that the difference for frame 100 is 0 as it (of course) equals itself.
#+NAME: fig:frame_difference_plot
[[./img/frame_difference_plot.png]]

An optimistic approach would be to label all the frames until the previous or next peak difference. For example, if the first event is ED, we could label all previous frames until the next peak difference as diastole. Likewise, if the final event is ES, we could label all following frames until the next peak difference as diastole. The peak can be found by finding the first frame whose difference is less than the one preceding it, i.e., when the difference is no longer increasing. This risks labeling too few frames if there is a local peak due to noise, but this problem can be mitigated by smoothing the summed absolute difference values. A gaussian blur with a kernel standard deviation of 5 was used to smooth the values.

We also risk labeling too many frames, adding wrongly labeled frames, because there are no guarantees that the peaks directly coincide with the change of phase. This problem can be mitigated by only including a certain percentage of frames leading up to the peak. We elect to include 75% of the frames leading up to the peaks.

An example of a smoothed absolute-difference curve with 75% of extrapolated frames highlighted is plotted in figure [[fig:extrapolated_labels]].

#+CAPTION: The same summed absolute frame difference plot as in figure [[fig:frame_difference_plot]], but smoothed using a gaussian blur with a kernel standard deviation of 5. The dashed lines represent phase-end events, and the frames in the light blue area are frames that have their phase labeled. Notice how the labeled frames' perimeter only extends 75% towards the peak on the right side. Also note that the gaussian blur causes the summed absolute frame difference for frame 100 to no longer be 0.
#+NAME: fig:extrapolated_labels
[[./img/extrapolated_labels.png]]

** Normalizing and Removing Invalid Videos
When labeling the frames, an assumption is that both events occur within the same cardiac cycle, though this is not always the case in the dataset. To filter out videos where the annotated end-phase events go beyond a single cycle, we again analyze the periodicity using a similar method to the one used in the previous section.

The summed absolute frame difference should at most have one peak if the frames are from the same cardiac cycle. If it has two or more peaks, it suggests that the labeled video contains more than one heartbeat and thus can not be adequately labeled. There are 19 such videos in total, and these are filtered out. A set of good and bad video label examples are visualized in figure [[fig:phase_diff_plots]].

#+CAPTION: The summed absolute frame difference between the first end-phase event and the frames until the next end-phase event. This should only be a half cardiac cycle, so there should be at most one peak. The upper plots show videos where the end-phase labels only cover one half cardiac cycle, while the bottom plots show videos with more than one cardiac cycle and thus have incorrect labels.
#+NAME: fig:phase_diff_plots
[[./img/phase_diff_plots.png]]


The videos already have the same size of 112-by-112, but the frames-per-seconds (FPS) differs. Luckily, most videos in the dataset have the same FPS ‚Äî almost 80% of the videos have exactly 50 FPS. The smallest FPS is 18, and the highest FPS is 138. See figure [[fig:echonet_fps_histogram]] for a histogram (logarithmic scale on the y-axis) of the different FPS values.

To normalize the videos with a much smaller FPS than 50, we would have to add information to them by inserting new frames. However, this may add unwanted bias to the data, and it is not obvious how to label the interpolated frames when the video goes from one phase to another. We would have to remove frames to normalize the videos with a much higher FPS. Unless the FPS is a multiple of 50, we risk introducing varying FPS to the video, which may confuse the model. For example, if a video has 75 FPS, we could opt to remove every third frame to make it 50 FPS, but this would make it seem like the heart moves slightly faster every third frame.

Because the Echonet dataset is so large, we opt to simply filter out all videos that have an FPS other than 50. Thus, we filter out another 2071 videos, leaving us with 7946 videos.

#+CAPTION: A histogram of the different FPS rates of the videos in the Echonet dataset. Note that the y-axis is on a logarithmic scale ‚Äî in fact, almost 80% of the videos have precisely 50 FPS.
#+NAME: fig:echonet_fps_histogram
[[./img/echonet_fps_histogram.png]]



** Training, Validation, Test Split
The dataset has already been split into three parts: one part for training the algorithm, one part for validation, and one for testing (i.e., presenting results). The percentage split is approximately 75% for training, 12.5% for validation, and 12.5% for testing. After filtering out videos as explained in the previous two sections, the split ratios remain approximately the same. We opt to continue using this split in this thesis.

A full Echonet-Dynamic dataset pipeline is visualized in figure [[fig:dataset_pipeline]].

#+CAPTION: A visualization of the data processing pipeline for the Echonet-Dynamic dataset, as described in the previous subsections. First, the ED- and ES-frames from the video are extracted from the volume tracings data. The frame with the biggest volume is ED; the other is ES. Next, more frame labels are extrapolated by looking at the absolute pixel differences between the ED- or ES-frame and the other frames of the video. Then, videos are filtered such that not more than one cardiac cycle is included in the labeled frames and all videos have 50 FPS. Finally, the videos are split randomly into three subsets: training, validation, and testing.
#+NAME: fig:dataset_pipeline
[[./img/dataset_pipeline.png]]




\chapter{Methodology}


* Environment Formulation
As described in section [[Reinforcement Learning]], a markov decision process (MDP), which is at the core of RL, can be described using four elements: the state space, the action space, the transition function, and the reward function. The states and actions dictate what information the agent receives from the environment and how it can, in turn, interact with the environment. The transition function defines the effect of actions on the environment. The reward function defines the goal of the agent.

** Binary Classification Environment
BCE is visualized in figure [[fig:binary_classification_environment_loop]]. After observing the current and adjacent frames, the agent takes an action predicting that the current frame is either in the diastole or systole phase and receives a reward dependent on its prediction before the environment moves the current frame one frame forward.

#+CAPTION: Visualization of the Binary Classification Environment loop. An agent sees the observation from the current frame and takes an action, either marking it as diastole or as systole, and gets back the reward and the observation for the next frame from the environment.
#+NAME: fig:binary_classification_environment_loop
[[./img/binary_classification_environment_loop.png]]

More formally, the observation $o_t$ at time $t$ is the current frame in the video prepended by the $N$ previous frames and the $N$ next frames. The shape of an observation is thus $(W, H, 2N+1)$. The agent takes the observation as-is and takes one of two actions: /Mark current frame as diastole/ or /Mark current frame as systole/. After taking an action $a_t$, the agent receives a reward $r_{t+1}$ and is presented with the next observation $o_{t+1}$. The current frame is moved one frame forwards after each action is taken, and the episode ends when there are no more labeled frames left.

Given that videos from the dataset are 112-by-112, the only two hyper-parameters for this setup are $N$ and the choice of reward function. Increasing $N$ means that the agent has access to more temporal information but at the cost of increased computational and memory requirements and a decrease in the number of videos with enough adjacent frames on either side. The number of valid videos for a given $N$ and the change in the number of valid videos is plotted in figure [[fig:n_valid_videos_for_n]]. As a starting point, $N$ was selected rather arbitrarily to be $3$. This means that an observation has the shape $(112, 112, 7)$, having $2\times 3 + 1 = 7$ channels.

#+CAPTION: The effect of $N$ on the size of the dataset. Left: the number of valid videos (videos with at least $N$ adjacent frames on either side) for the whole dataset. Right: the change in the number of valid videos per $N$ for the whole dataset.
#+NAME: fig:n_valid_videos_for_n
[[./img/n_valid_videos_for_n.png]]









** Reward Function Design
 The standard metric for this task is the average absolute frame difference (aaFD), as defined in equation [[eqn:aafd]]. aaFD measures the precision and accuracy of predictions by measuring the frame difference between each ground truth event $y_t$ and the corresponding prediction $\hat{y}_t$ generated by the model ‚Äî a lower aaFD meaning that the model is making fewer errors. $t$ is the index of a specific event, of which there are $N$ in total.

 #+NAME: eqn:aafd
 \begin{equation}
 aaFD=\frac{1}{N}\sum^N_{t=1}|y_t-\hat{y}_t|
 \end{equation}

 One weakness of aaFD is that it is only defined when there are an equal number of predicted events as there are ground truth events. This is not always the case, as an imperfect model may predict more or fewer events. A generalized aaFD ($GaaFD_1$) was considered for a metric instead, calculated as the average frame difference between each predicted event and its nearest ground truth event as in equation [[eqn:aafd_generalized_1]], having the property that it converges towards the true aaFD as the model improves. In equation [[eqn:aafd_generalized_1]] $\hat{N}$ is the number of predicted events and $\mathcal{C}(y, \hat{y})$ is the frame difference between the predicted event to the /closest/ ground truth event of the same type. For cases where there are more predicted events than there are ground truth events, $GaaFD_1$ would, as is rational, give a worse score. However, for cases with fewer predicted events than ground truth events, $GaaFD_1$ would give a score that does not reflect its inability to predict all events.

 #+CAPTION: $\mathcal{C}(y, \hat{y}_t)$ is the closest ground truth event from the predicted event $\hat{y}_t$. $\hat{N}$ is the number of predicted events.
 #+NAME: eqn:aafd_generalized_1
 \begin{equation}
 GaaFD_1=\frac{1}{\hat{N}}\sum^{\hat{N}}_{t=1}|\mathcal{C}(y, \hat{y}_t)-\hat{y}_t|
 \end{equation}

Similarly, we could base it on the ground truth events and take the distance to the nearest predicted event, $GaaFD_2$, as in equation [[eqn:aafd_generalized_2]], we get the opposite problem ‚Äî too many predicted events are not reflected negatively in the score.

 #+CAPTION: $\mathcal{C}(y_t, \hat{y})$ is the closest predicted event from the ground truth event $y_t$.
 #+NAME: eqn:aafd_generalized_2
 \begin{equation}
 GaaFD_2=\frac{1}{N}\sum^N_{t=1}|y_t - \mathcal{C}(y_t, \hat{y})|
 \end{equation}

 By combining $GaaFD_1$ and $GaaFD_2$ as in equation [[eqn:aafd_generalized]] we mitigate these problems while maintaining the convergence property.

 #+NAME: eqn:aafd_generalized
 \begin{equation}
 GaaFD = \frac{1}{N+\hat{N}}(\sum^N_{t=1}|y_t - \mathcal{C}(y_t, \hat{y})| + \sum^{\hat{N}}_{t=1}|\mathcal{C}(y, \hat{y}_t)-\hat{y}_t|)
 \end{equation}

 Using negative GaaFD (negative because we wish to minimize it) as a reward function for RL means optimizing the agent directly for our main metric aaFD. However, it has one final flaw: it is only defined on whole episodes. This means that the agent has to run an entire episode before getting a reward, making the reward signal sparse.

 Instead, we could frame the problem as a simple classification problem where the agent must classify individual frames as either ED, ES, or neither. This allows us to give a reward at each step depending on whether the prediction was correct or not. One problem with this approach is that there is a heavy class imbalance because most frames are neither ED nor ES. A solution to this is to instead predict the phase, either diastole or systole, as it is trivial to find ED and ES from the phase by finding the frames where it transitions from one to the other.

From this, we can define a simple reward function $R_{simple}$ that gives a reward of $1$ if the predicted phase was correct and $-1$ if it was incorrect, as seen in equation [[eqn:simple_reward]]. The information that the agent receives from the reward signal $R_{simple}$ is slightly different from the one defined through GaaFD, as GaaFD penalizes predictions that are more wrong heavier than those that are close to the ground truth.

 #+NAME: eqn:simple_reward
 \begin{equation}
   R_{simple}(s, a) \triangleq
     \left\{
	     \begin{array}{ll}
		     1 & \mbox{if } \text{phase}(s)=a \\
  	  	 -1 & \mbox{if } \text{phase}(s)\neq a
	     \end{array}
     \right\}
 \end{equation}

We can make the reward signal more similar to GaaFD by defining it in terms of the distance to the nearest predicted phase, as seen in equation [[eqn:proximity_reward]], where $d(s,a)$ is the distance in frames from the current state $s$ to the nearest frame that has the predicted phase $a$.

 #+NAME: eqn:proximity_reward
 \begin{equation}
   R_{proximity}(s, a) \triangleq -d(s, a)
 \end{equation}



* Frameworks and Libraries
The code to train and run the agent is written in Python because of its ML and data-processing ecosystem. The main framework for data-processing is JAX \cite{bradbury_jax_2018}. Other frameworks considered were Tensorflow \cite{abadi_tensorflow_2015} and PyTorch \cite{paszke_pytorch_2019}. A list of the most important ones can be found in table \ref{tbl:rl_libs}.

\begin{table}[htbp]
\caption{\label{tbl:rl_libs}A collection of the most important libraries used in the project.}
\centering
\begin{tabular}{| p{2.75cm} | p{10cm} |}
Library & Description\\
\hline
jax & Main data-processing framework. Provides autodifferentiation, vectorization, Just-In-Time (JIT) compilation, and more \cite{bradbury_jax_2018}\\
gym & An interface for defining RL environments \cite{brockman_openai_2016}\\
dm-haiku & A neural network library for JAX \cite{hennigan_haiku_2020}\\
optax & A gradient processing and optimization library for JAX \cite{hessel_optax_2020}\\
rlax & Building blocks for building RL agents \cite{babuschkin_deepmind_2020}\\
dm-acme & Distributed RL agent implementations and building blocks \cite{hoffman_acme_2020}\\
dm-reverb & A database for storing and sampling experience replay \cite{cassirer_reverb_2021}\\
dm-launchpad & A library for defining and creating distributed systems \cite{yang_launchpad_2021}\\
Scikit-learn & A collection of machine learning algorithms. In this project it is mostly used for calculating metric \cite{pedregosa_scikit-learn_2011}\\
\end{tabular}
\end{table}


* Agent Architecture
Deep Q-Network was selected for the RL agent architecture. DQN is a well-established method for scaling up RL by approximating the expected returns of an action in a given state using a (deep) neural network. It is also simple to train distributedly as it is off-policy, enabling us to separate the algorithm into a learner and multiple agents, as explained in a following section. 

We take advantage of a few additions to the original DQN algorithm: Prioritized Replay, N-step returns, and Double Q-Learning. An $\epsilon$ -greedy policy is used for facilitating exploration.


** Neural Network
The neural network that approximates the Q-function is inspired by the original Atari DQN paper \cite{mnih_human-level_2015}. It has two convolutional layers and two fully connected layers. A ReLU activation layer follows each layer except for the last one. The first convolutional layer has 16 output channels, a kernel size of 8-by-8, and a stride of 4. The second has 32 output channels, a kernel size of 4-by-4, and a stride of 2. The data is flattened before being passed to the fully connected layers. The first fully connected layer has an output size of 256. The final layer has two outputs, each representing the estimated value of taking one of the actions, given the input state. In total there are $1\,621\,810$ parameters. The network is visualized in figure [[fig:simple_dqn_network]].

#+CAPTION: A visualization of the simple DQN-Atari-paper-inspired CNN.
#+NAME: fig:simple_dqn_network
[[./img/simple_dqn_network.png]]

** Loss Function and Optimizer
The loss function is the Double Q-Learning loss where the TD-error is calculated with respect to another Q-network. Because of this, we have to keep track of two sets of network parameters: one for the selector Q-network and one for the estimator Q-network. Huber loss \cite{huber_robust_1964} is applied to the TD-error such that the L2 loss becomes linear after a certain threshold. In addition, the loss is weighted with respect to the prioritized replay importance weights.

The Adam optimizer \cite{kingma_adam_2017} is used to update the selector parameters, and the target network parameters are updated to equal the selector parameters every 100 gradient descent steps.


** Distributed Training
 As mentioned, DQN lends itself nicely to distributed training. In this project, this is achieved through a library called Acme \cite{hoffman_acme_2020}. At the center of Acme is another library called Reverb \cite{cassirer_reverb_2021}. Reverb is a database for storing experience replay samples that lets us insert and sample experiences independently. If we separate the learning step and the acting step of the algorithm, Reverb can be used as the communication point between the two. One or more actors, possibly on different machines, can generate experience samples and insert them into the Reverb experience replay database. A learner, also possibly on a different machine, can sample from it to perform gradient descent. The actors and the learner do not need to know about each other, except when an actor needs to update its parameters, in which case it needs to query the learner for the latest trained parameters. It is also trivial to add one or more evaluators that can run in parallel and that only need to query the learner for the latest trained parameters. Inter-process communication is facilitated by a third library called Launchpad \cite{yang_launchpad_2021}.

 #+CAPTION: An illustration of the distributed RL training system. Each pink node runs in a separate Python process, and each blue arrow is an inter-process function call facilitated by Launchpad.
 #+NAME: fig:distributed_rl_training
 [[./img/distributed_rl_training.png]]

There is a balance between how fast experience samples should be added to the experience replay and how fast the learner should sample them. If the learner samples faster than the actors can generate new samples, the network will be trained using trajectories generated from outdated policies. If the actors generate new samples much faster than the learner can sample, then we are wasting computer resources.

 Reverb helps maintain this balance through rate limiters. We use a rate limiter that tries to maintain a specified ratio between insertions and samples, blocking either the actors from inserting new samples or the learner from sampling if the ratio differs too much. For example, using a samples-per-insert ratio of 2 means that, on average, each insertion made by an actor will be sampled twice. A ratio of 0.5 means that, on average, each insertion will be sampled half a time ‚Äî i.e., there are twice as many insertions as there are samples.





* Evaluation
During training, the updated parameters of the model are continuously evaluated using GaaFD on 50 videos, randomly selected each time, from the validation set. Smoothing is applied to the learning curves using a Gaussian filter with a kernel standard deviation of 10 to compensate for the low sample size for each point. The best parameters are selected by finding the parameters that produce the lowest GaaFD during training for the smoothed GaaFD learning curve. 

The primary evaluation metric for the trained model is aaFD. However, some videos may not receive the same number of predicted events as there are ground truth events, so aaFD is undefined. Because of this, aaFD is only reported for videos where it is defined. Additionally, the percentage of videos with a defined aaFD is reported. The corresponding ground truth event to each predicted event is chosen to be the closest one, and we can therefore use GaaFD, as defined in equation [[eqn:aafd_generalized]], for calculating aaFD.

It may also be interesting to see the density plots of GaaFD for all videos and compare the performance of the agent on ED- and ES-frames individually. The density plots used are an approximation of the continuous distribution of GaaFD. A histogram may also be used, but density plots were found to be easier to compare using density plots. They are created using gaussian kernel estimation (KDE) \cite{scott_multivariate_1992}. The kernel bandwidth is automatically selected using Scott's rule, the default selection method for SciPy's KDE implementation.

Because the RL problem formulation is similar to a regular binary classification problem, accuracy and balanced accuracy are also reported. Accuracy and balanced accuracy are defined on frame phase predictions instead of end-phase events. Accuracy is simply the percentage of correctly labeled frames, as defined in equation [[eqn:accuracy]], where $1(y=\hat{y})$ is the indicator function. Given that there is a class imbalance between diastole and systole frames, balanced accuracy gives a more representative score of the actual model performance. Balanced accuracy weights systole frames accuracy higher than diastole frames and is defined in equation [[eqn:balanced_accuracy]]. $TP$, $FP$, $TN$, and $FN$ stand for "true positives", "false positives", "true negatives", and "false negatives", respectively. It is also defined as the average between the sensitivity and the specificity. The balanced accuracy score is also rescaled such that it gives a score in the range $[-1, 1]$, where $0$ means that the model's predictions are random, and $-1$ and $1$ mean that the predictions are all incorrect or all correct, respectively.

#+NAME: eqn:accuracy
\begin{equation}
\text{accuracy}(y, \hat{y}) = \frac{1}{N}\sum_{i=0}^N 1(\hat{y}_i=y_i)
\end{equation}


#+NAME: eqn:balanced_accuracy
\begin{equation}
\text{balanced-accuracy}(y, \hat{y}) = \frac{1}{2}(\frac{TP(y, \hat{y})}{TP(y, \hat{y})+FN(y, \hat{y})} + \frac{TN(y, \hat{y})}{TN(y, \hat{y})+FP(y, \hat{y}}
\end{equation}

Models are also evaluated on their inference time ‚Äî how long it takes to make predictions for a video. To use a trained model, one can use the Q-network directly, without instantiating a gym environment or using an $\epsilon$ -greedy policy. The Q-network outputs the expected returns of taking either action, so picking the action with the highest output is the same as following a greedy policy. The Q-network can be evaluated on individual frames or on the video as a whole, where all the frames are combined into a single batch. Evaluating each frame individually enables incorporating the model into a pipeline of streaming frames, of which one step is predicting the current cardiac phase. Evaluating the whole video as a batch is generally faster as it gets away with less IO overhead of sending data back and forth between the CPU and the GPU.

Batching the frames of a video may require more JIT compilation with JAX. This is because, to speed the network up significantly, it is JIT-compiled to XLA, but JIT-compiled functions require that the shape of the data remain the same. If the shape of the data is not the same, e.g., if we are evaluating two videos with a different number of frames as two different batches, the function will be recompiled, adding overhead. This could be solved by fixing the batch size to a constant number. For videos with fewer frames than the batch size or with a number of frames that can not be split into equal chunks of the batch size, frames filled with zeros can be added. These extra frames create needless work on the GPU but do not require recompilation.

Inference time is evaluated using single frame-inference and batched-frames inference with a batch size of 128 on the CPU and GPU. Additionally, IO overhead is reported by comparing the average processing time when sending the data to GPU for each call versus pre-placing the data on the GPU. The average run time is calculated by taking the elapsed time, averaged over 1000 calls.

Finally, models are evaluated on how long it took to train them in clock time and the number of SGD steps performed.




* Selection of Hyper-Parameters
** Generalized Average Absolute Frame Difference Reward Function
Using GaaFD directly as the reward function has the benefit that we are directly optimizing the agent for the primary performance metric aaFD, as defined in equation [[eqn:aafd]]. However, as discussed in section [[Reward Function Design]], a weakness is that it is only defined at the end of an episode, making the reward signal very sparse. The agent will only get a reward at the last step of an episode, which, on average, lasts for 50 steps.

To solve for reward-sparsity, we use multistep bootstrapping with a value of $N=200$. An episode is automatically terminated once it reaches 200 steps [[fn::Though in the case of the simple binary classification environment, this will never happen because no video has this many frames] so this will, in practice, mean that the agent is trained using the Monte Carlo method.
 
We also set the discount value $\gamma=1.0$, which means that an agent tries to maximize all future rewards. A value of $\gamma < 1.0$ means that the calculated returns will be noisier and harder to predict because the discounted returns calculated for steps earlier in an episode would have a lower value than those calculated closer to the end.

The expected returns are assumed to be very sensitive to the current policy as a correctly selected next action's returns may be jeopardized by future wrongly selected actions. This would make it hard for the agent to extract information about which actions were wrong and which were correct. Because of this, we opt to use very low values of the exploration parameter $\epsilon$, as higher values of $\epsilon$ means that actions will be selected at random more often, making the expected returns harder to predict. Three values are tested for the exploration hyper-parameter $\epsilon$: $\epsilon=0.0$, $\epsilon=0.01$, and $\epsilon=0.1$. The agents were allowed to train until they visually reached a plateau. A full list of the hyper-parameters used is listed in table [[tbl:gaafd_hyper_params]] (most relevant ones are highlighted).

#+NAME: tbl:gaafd_hyper_params
| Hyper parameter              | Value                              |
|------------------------------+------------------------------------|
| *Epsilon*                    | $\{0.0, 0.01, 0.1\}$               |
| *Discount*                   | $1.0$                              |
| *N (N-step bootstrapping)*   | $\infty$                           |
| Target update period         | $100$                              |
| Importance sampling exponent | $0.2$                              |
| Priority exponent            | $0.6$                              |
| Number of actors             | $8$                                |
| Min replay size              | $10\,000$                          |
| Max replay size              | $250\,000$                         |
| Samples per insert ratio     | $0.5$                              |
| Optimizer                    | Adam with default parameters       |
| Huber loss parameter         | $1.0$                              |
| Learning rate                | $1^{-4}$                           |
| Gradient descent steps       | $\{100\,000, 150\,000, 200\,000\}$ |
| Batch-size                   | $128$                              |




** Simple- and Proximity-Based Reward Functions
Using reward functions based on each phase prediction gets around the reward sparsity problem of using GaaFD as the reward function. Two more reward functions are explored: a simple reward function $R_{simple}$, as defined in equation [[eqn:simple_reward]], and an proximity-based reward function $R_{proximity}$, as defined in equation [[eqn:proximity_reward]]. This makes it quite similar to a supervised regression problem where we want to learn the Q-values given an observation and an action. The returns only depend on the current action and not on all the actions in an episode, as we saw with the GaaFD reward function. As a result, it is assumed that the optimal discounting factor is $\gamma=0.0$, meaning that the returns are calculated using only the immediate reward. A discount value of $\gamma > 0.0$ would make expected future returns predictions depend more on the current policy, adding noise to the target values until the policy converges.

Unless discounting is not zero, there will be no need for bootstrapping, and we can ignore N-step bootstrapping for these reward functions by setting $N=1$.

Since an action does not affect future states, exploration is not as important. Instead, we can view the exploration variable $\epsilon$ as affecting how input/label pairs are sampled. An exploration value of $\epsilon=1.0$ means that actions are sampled uniformly, and a value of $\epsilon=0.0$ means that actions are sampled based on how good it is assumed to be. Three values are tested for the exploration hyper-parameter $\epsilon$: $\epsilon=0.1$, $\epsilon=0.5$, and $\epsilon=1.0$. The agents were trained for $200\,000$ SGD steps. A full list of the hyper-parameters used for experiments with reward functions $R_{simple}$ and $R_{proximity}$ is listed in table [[tbl:r1r2_hyper_params]] (most relevant ones are highlighted).

#+NAME: tbl:r1r2_hyper_params
| Hyper parameter              | Value                        |
|------------------------------+------------------------------|
| *Epsilon*                    | $\{0.1, 0.5, 1.0\}$          |
| *Discount*                   | $0.0$                        |
| *N (N-step bootstrapping)*   | $1$                          |
| Target update period         | $100$                        |
| Importance sampling exponent | $0.2$                        |
| Priority exponent            | $0.6$                        |
| Number of actors             | $8$                          |
| Min replay size              | $10\,000$                    |
| Max replay size              | $250\,000$                   |
| Samples per insert ratio     | $0.5$                        |
| Optimizer                    | Adam with default parameters |
| Huber loss parameter         | $1.0$                        |
| Learning rate                | $1^{-4}$                     |
| Gradient descent steps       | $200\,000$                   |
| Batch-size                   | $128$                        |





* Incorporating Search
Although RL is designed to be able to perform a search through an unknown state space, in the previous setup, there is no exploration as previous actions do not affect future actions. Therefore, there is no reason to believe that RL will outperform a carefully designed supervised learning approach. By transforming the problem to one that requires search, we will have a problem not trivially solved by supervised learning but where RL can shine. Though this may seem like straightening a screw to make it work with a hammer, there may be unforeseen benefits. Of great importance to ML is to represent the problem space such that it is easy for an algorithm to learn from it. Perhaps there is an optimal representation of the problem of ED-/ES-detection that also happens to require search?

** Temporal Search
We could formulate the problem as a search in time where the agent must learn to move the current frame towards the end-phase event. The agent sees the current frame and some number of previous and following frames and can either move the current frame backward or forwards. The agent can be rewarded with 1 if it moves a step closer to the nearest end-phase frame and -1 if it moves away from it.

There are a handful of issues with this approach. *Issue 1*: we would have to train two different agents: one for ED and one for ES. *Issue 2*: there is no terminal state, and the episodes can run forever. *Issue 3*: there will be ambiguity in what frame the agent truly predicts as the end-phase because it will likely show oscillating behavior around the predicted frame. *Issue 4*: we would have to run multiple agents at different points in the video to find all end-phase events, and it is not obvious how to do so.

*Issue 2* and *issue 3* can be partially solved by including a third action for marking the current frame and ending the episode, though this may still lead to the agent getting stuck in an endless loop of going back and forth. We could also keep just the two actions but terminate the episode once the agent starts showing oscillating behavior, as in \cite{alansary_evaluating_2019}, as this indicates that it has found the predicted frame. The problem with this is that the final predicted frame would be ambiguous as we do not know which of the two frames the agent oscillates between is the actual predicted frame. However, using DQN, we could peek at the Q-values and pick the frame where the expected reward of taking the action with the maximum expected reward is the lowest. *Issue 4* may be solved by starting an agent from each frame, though this would increase the computational requirements of the algorithm.


** Spatial Search
Instead of searching through the video frames, we could let the agent search spatially in the video. In this formulation, the agent only has access to a part of the images while predicting the phase of frames. Like landmark detection tasks, it can move its focus around in the image, the hope being that it can discover parts of the video, which makes it easier to identify the correct phase. This can be seen as reducing the space and memory requirements at the cost of speed, as the agent has to process a smaller part of the image but may explore multiple steps before making a prediction.

One option is to look at a region of interest (ROI) around a point that the agent can move. Building upon the simple binary classification environment described in previous sections, this would add four new actions: move up, move down, move left, and move right. This is visualized in figure [[fig:roi_exploration]]. Because we reduce the size of the observations, we could either trade it for reduced memory usage or for including more temporal information in terms of included adjacent frames.

 #+CAPTION: A region of interest (ROI) is given to the agent, which it can then move around to explore.
 #+NAME: fig:roi_exploration
 [[./img/roi_exploration.png]]

Another option is to take inspiration from m-mode imaging used in ultrasound. We can define a synthetic m-mode image in terms of a line in the video, where it shows how the pixels along this line change over time. A video can be seen as a 3D data cube consisting of width, height, and time. When using the synthetic m-mode technique, width and height are replaced by the line, effectively removing one spatial dimension while keeping the temporal dimension intact. The m-mode can be seen as taking a 2D slice of the video, as seen in figure [[fig:m_mode_cube]]. This synthetic m-mode exploration formulation adds six new actions: move up, move down, move left, move right, rotate left, and rotate right. M-mode imaging is also a well-established imaging mode in clinical settings, so this is the method that we want to explore further.

 #+CAPTION: An m-mode image is an intersecting plane in 3D "video space".
 #+NAME: fig:m_mode_cube
 [[./img/m_mode_cube.png]]

Moving the synthetic m-mode line up, down, left, or right is done relative to its rotation. We call this local translation, different from global translation, where the movement is independent of the rotation of the line. Local and global translation is visualized in figure [[fig:local_vs_global_mmode_translation]]. Using local translation is presumed to add some rotational invariance, as the m-mode line can counteract the rotation of the video itself without changing the perceived m-mode effects of translation. This also makes the effects of the up- and down-translations trivial, as seen in figure [[fig:m_mode_vertical_movement_effect]] ‚Äî independent of rotation, it simply shifts the m-mode image down or up, respectively.

 #+CAPTION: Global (to the left) versus local (to the right) translation. Local translation means that the movement depends on the direction of the m-mode line.
 #+NAME: fig:local_vs_global_mmode_translation
 [[./img/local_vs_global_mmode_translation.png]]

#+CAPTION: Moving the synthetic m-mode line up or down using local translation changes the resulting image very little ‚Äî it simply translates it up or down, as indicated by the blue arrows. To the left: an overview image of a video with the line added on top. To the right: the resulting synthetic m-mode image.
#+NAME: fig:m_mode_vertical_movement_effect
[[./img/m_mode_vertical_movement_effect.png]]

* M-Mode Binary Classification Environment
We formulate the m-mode binary classification environment using a synthetic m-mode search space scheme. The agent can make one of 8 actions: /Mark current frame as diastole/, /Mark current frame as systole/, /Rotate line/ (left or right), /Move line along its pointing direction/ (up or down), and /Move line perpendicular to its pointing direction/ (left or right). The observation includes the synthetic m-mode image of the current line position. However, we also want to give the agent explicit information about what it would look like if it moved or rotated the line and a history of the latest actions.

An observation thus consists of the synthetic m-mode image for three different rotations (rotated left, not rotated at all, and rotated right) and for three different perpendicular movements (moved to the left, not moved at all, moved to the right), for a total of 9 synthetic m-mode images. Up and down line movements are not included as additional channels because they do not provide as much information to the agent, as seen in figure [[fig:m_mode_vertical_movement_effect]]. The synthetic m-mode image is created by interpolating the line across the video using nearest-neighbor interpolation. An overview image consisting of the average of the first 50 frames and the current position of the synthetic m-mode line is also included in the observation. Lastly, we include the last five actions taken as a one-hot encoded array of shape $(5, 8)$ ‚Äî 8 being the number of possible actions. Observations are thus a tuple of:

1. An "overview" image of shape $(W, H, 2)$
2. A synthetic m-mode image of shape $(T, L, 9)$
3. An action history array of shape $(5, 8)$

$W$ and $H$ are the width and height of the video, respectively, and $T$ and $L$ are the number of frames (amount of temporal information) and length of the line, respectively.

At the start of an episode, the line is placed randomly within a bounding box. This is to force the agent to learn to explore instead of learning to predict the phase from a common starting position. First, the line is centered, facing upwards. Then it is translated in the direction it is facing by a random amount, sampled uniformly from the interval $[-0.1H, 0.1H]$. Then it is translated perpendicular to the direction it is facing by a random amount, sampled uniformly from the interval $[-0.1W, 0.1W]$. This ensures that the line's center is never more than $0.1H$ units away in the y-direction or $0.1W$ units away in the x-direction from the image's center. Lastly, it is rotated by an angle sampled uniformly from the interval $[-\frac{\pi}{2}, \frac{\pi}{2}]$ radians. If a line is somehow generated outside of the video bounds, a new line is generated.

The random starting positions were verified to be reasonable through visual inspection of a sample of $1\,000$ lines, as seen in figure [[fig:m_mode_line_starting_positions]]. An episode ends once the agent has predicted the phase of every frame or for a maximum of 200 steps. We have to cut the episode off at 200 steps because the agent may now move indefinitely. Given that the average video length is 50 frames, 200 steps should give the agent ample time to find the best synthetic m-mode line position in most cases.

#+CAPTION: The union of 100 randomly sampled m-mode lines.
#+NAME: fig:m_mode_line_starting_positions
[[./img/m_mode_line_starting_positions.png]]

The reward function is the same as in the simple binary classification environment but with some modifications. With MMBCE, the agent may move the line out of the bounds of the video. It may also get stuck in an infinite loop of actions. The agent is determined to be stuck in a loop if the line ends up in a previously visited position and there are no phase predictions since then. If the agent moves the line out of bounds or gets stuck in a loop, the line is moved to a new random position, and the agent is given a reward of $-1$.














** Agent Architecture
We keep the same base architectures as in the simple binary classification environment, but we also need to accommodate the overview image and action history array. This is done by concatenating the result of passing all three arrays through their corresponding neural networks. The synthetic m-mode and overview images are passed through the Atari DQN-paper-inspired CNN that is visualized in figure [[fig:simple_dqn_network]], but with the final output layer removed in order to accommodate concatenation. The action-history array is flattened before being passed into a fully connected layer with 32 outputs, followed by a ReLU activation layer. After concatenating the three results, they are passed through yet another fully connected layer of 64 outputs and a ReLU activation layer before being passed through a final fully connected layer with two outputs. The network is visualized in figure [[fig:simple_dqn_network_m_mode]].

#+CAPTION: The network architecture of the m-mode agent. An observation consists of three parts. Each part is processed independently by a neural network before being concatenated and used to produce the approximated Q-values.
#+NAME: fig:simple_dqn_network_m_mode
[[./img/simple_dqn_network_m_mode.png]]










\chapter{Experiments and Results}


* Performance Metrics
The performance metrics is reported in the following four tables. Tables [[tbl:gaafd_best_models_sgd_steps]], [[tbl:r1_best_models_sgd_steps]] and [[tbl:r2_best_models_sgd_steps]] presents the performance of agents traines using $R_{GaaFD}$, $R_{simple}$ and $R_{proximity}$, respectively. Lastly, table [[tbl:best_models_comparison]] presents a comparison between the best models for each reward function.

Out of all the trained models, the model trained using $R_{simple}$ with a value of $\epsilon=0.5$ performed the best.

#+CAPTION: Performance of agents trained using GaaFD as the reward function on the test dataset.
#+NAME: tbl:gaafd_best_models_sgd_steps
|                     | $\epsilon=0.1$ | $\epsilon=0.01$ | $\epsilon=0.0$ |
|---------------------+----------------+-----------------+----------------|
| Best model SGD step | $167\,336$     | $136\,996$      | $95\,616$      |
| GaaFD               | $5.84$         | $4.59$          | $3.68$         |
| GaaFD ED            | $5.69$         | $4.84$          | $3.74$         |
| GaaFD ES            | $5.83$         | $4.20$          | $3.50$         |
| % valid aaFD        | $63.71\%$      | $70.33\%$       | $76.63\%$      |
| aaFD                | $3.51$         | $2.71$          | $2.43$         |
| Accuracy            | $0.82$         | $0.86$          | $0.88$         |
| Accuracy diastole   | $0.90$         | $0.91$          | $0.94$         |
| Accuracy systole    | $0.69$         | $0.75$          | $0.77$         |
| Balanced accuracy   | $0.58$         | $0.67$          | $0.70$         |

#+CAPTION: Performance of agents trained using $R_{simple}$ as the reward function on the test dataset.
#+NAME: tbl:r1_best_models_sgd_steps
|                     | $\epsilon=0.1$ | $\epsilon=0.5$ | $\epsilon=1$ |
|---------------------+----------------+----------------+--------------|
| Best model SGD step | $6\,220$       | $10\,960$      | $8\,964$     |
| GaaFD               | $2.52$         | $2.46$         | $2.57$       |
| GaaFD ED            | $2.48$         | $2.43$         | $2.52$       |
| GaaFD ES            | $2.47$         | $2.41$         | $2.55$       |
| % valid aaFD        | $79.30\%$      | $80.26\%$      | $76.95\%$    |
| aaFD                | $1.71$         | $1.69$         | $1.69$       |
| Accuracy            | $0.91$         | $0.91$         | $0.91$       |
| Accuracy diastole   | $0.93$         | $0.93$         | $0.93$       |
| Accuracy systole    | $0.87$         | $0.88$         | $0.88$       |
| Balanced accuracy   | $0.80$         | $0.81$         | $0.81$       |

#+CAPTION: Performance of agents trained using $R_{proximity}$ as the reward function on the test dataset.
#+NAME: tbl:r2_best_models_sgd_steps
|                     | $\epsilon=0.1$ | $\epsilon=0.5$ | $\epsilon=1$ |
|---------------------+----------------+----------------+--------------|
| Best model SGD step | $107\,936$     | $6\,880$       | $7\,096$     |
| GaaFD               | $2.55$         | $2.56$         | $2.63$       |
| GaaFD ED            | $2.52$         | $2.56$         | $2.67$       |
| GaaFD ES            | $2.50$         | $2.48$         | $2.52$       |
| % valid aaFD        | $78.87\%$      | $79.40\%$      | $76.95\%$    |
| aaFD                | $1.74$         | $1.80$         | $1.71$       |
| Accuracy            | $0.91$         | $0.91$         | $0.91$       |
| Accuracy diastole   | $0.94$         | $0.93$         | $0.93$       |
| Accuracy systole    | $0.86$         | $0.87$         | $0.88$       |
| Balanced accuracy   | $0.80$         | $0.80$         | $0.81$       |

#+CAPTION: Performance of the best agent for each explored reward function on the test dataset. The best agent was selected by the best GaaFD score.
#+NAME: tbl:best_models_comparison
|                     |    $R_{GaaFD}$ | $R_{simple}$   | $R_{proximity}$ |
|---------------------+----------------+----------------+-----------------|
| $\epsilon$          | $\epsilon=0.0$ | $\epsilon=0.5$ |  $\epsilon=0.1$ |
| Best model SGD step |         95 616 | *10 960*       |         107 936 |
| GaaFD               |           3.68 | *2.46*         |            2.55 |
| GaaFD ED            |           3.74 | *2.43*         |            2.52 |
| GaaFD ES            |           3.50 | *2.41*         |            2.50 |
| % valid aaFD        |         76.63% | *80.26%*       |          78.87% |
| aaFD                |           2.43 | *1.69*         |            1.74 |
| Accuracy            |           0.88 | *0.91*         |          *0.91* |
| Accuracy diastole   |         *0.94* | 0.93           |          *0.94* |
| Accuracy systole    |           0.77 | *0.88*         |            0.86 |
| Balanced accuracy   |           0.70 | *0.81*         |            0.80 |


* The Impact of Epsilon on Average Absolute Frame Difference
Lower values of $\epsilon$ yield a better GaaFD score when using $R_{GaaFD}$ as the reward function. This is best seen in figure [[fig:gaafd_best_model_perf]]. There is no consistent difference between GaaFD on ED- or ES-frames individually, but there is less difference in performance on ES-frames between the training split and the test split, as seen in figure [[fig:gaafd_best_model_ed_es_perf]]. Figure [[fig:gaafd_mismatched_predictions]] shows that lower values of $\epsilon$ also reduce the mismatch between the number of predicted versus ground truth events. This was also clearly seen in table [[tbl:gaafd_best_models_sgd_steps]], where a value of $\epsilon=0.0$ predicted the correct number of events 77% of the time, while $\epsilon=0.1$ and $\epsilon=0.01$ yielded 64% and 70%, respectively. We also see a significant "bump" when the difference between predicted and ground truth events is two.

#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using GaaFD as the reward function. The left plot compares all three models on the test split. The middle plot compares all three models on the train split. The right plot shows the difference between the two as a means to visualize model overfitting.
#+NAME: fig:gaafd_best_model_perf
[[./img/gaafd_best_model_perf.png]]

#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using GaaFD as the reward function, only accounting for either ED- or ES-events individually. The upper row compares the performance of ED and ES for each model. The bottom row shows the difference in GaaFD-density on the test-set versus the train-set as a means to visualize model overfitting.
#+NAME: fig:gaafd_best_model_ed_es_perf
[[./img/gaafd_best_model_ed_es_perf.png]]

#+CAPTION: The difference between the number of predicted events and the number of ground truth events for each model when using GaaFD as the reward function. Most predictions produce the same number of predicted events as ground truth, e.g., the model with $\epsilon=0$ produces the correct number of events 77% of the time, also shown in table [[tbl:gaafd_best_models_sgd_steps]].
#+NAME: fig:gaafd_mismatched_predictions
[[./img/gaafd_mismatched_predictions.png]]




The choice of $\epsilon$ did not matter as much for agents trained using $R_{simple}$ or $R_{proximity}$, compared to those trained using $R_{GaaFD}$, as seen in figures [[fig:r1_gaafd_performance]] and [[fig:r2_gaafd_performance]]. As with $R_{GaaFD}$, $R_{simple}$ and $R_{proximity}$ showed little consistent difference in performance between gaaFD on ED- or ES-frame individually, as seen in figures [[fig:r1_edes_gaafd_performance]] and [[fig:r2_edes_gaafd_performance]]. Figure [[fig:r1r2_num_pred_mismatch]] further shows that there is little difference between values of $\epsilon$ for predicting the correct number of events as the number of ground truth events, both for $R_{simple}$ and $R_{proximity}$.


#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.5$, and $\epsilon=1.0$) when using $R_{simple}$ as the reward function. The left plot compares all three models on the test split. The middle plot compares all three models on the train split. The right plot shows the difference between the two as a means to visualize model overfitting.
#+NAME: fig:r1_gaafd_performance
[[./img/r1r2_res/r1_gaafd_performance.png]]

#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{simple}$ as the reward function, only accounting for either ED- or ES-events individually. The upper row compares the performance of ED and ES for each model. The bottom row shows the difference in GaaFD-density on the test-set versus the train-set as a means to visualize model overfitting.
#+NAME: fig:r1_edes_gaafd_performance
[[./img/r1r2_res/r1_edes_gaafd_performance.png]]

#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.5$, and $\epsilon=1.0$) when using $R_{proximity}$ as the reward function. The left plot compares all three models on the test split. The middle plot compares all three models on the train split. The right plot shows the difference between the two as a means to visualize model overfitting.
#+NAME: fig:r2_gaafd_performance
[[./img/r1r2_res/r2_gaafd_performance.png]]

#+CAPTION: Gaussian KDE of the GaaFD-performance for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{proximity}$ as the reward function, only accounting for either ED- or ES-events individually. The upper row compares the performance of ED and ES for each model. The bottom row shows the difference in GaaFD-density on the test-set versus the train-set as a means to visualize model overfitting.
#+NAME: fig:r2_edes_gaafd_performance
[[./img/r1r2_res/r2_edes_gaafd_performance.png]]


#+CAPTION: The difference between the number of predicted events and the number of ground truth events for each model when using $R_{simple}$ (left) and $R_{proximity}$ (right) as the reward function. Most predictions produce the same number of predicted events as ground truth, e.g., the model with $\epsilon=0.5$ and $R_{simple}$ as the reward function produces the correct number of events 80% of the time, which can also be seen in table [[tbl:r1_best_models_sgd_steps]].
#+NAME: fig:r1r2_num_pred_mismatch
[[./img/r1r2_res/r1r2_num_pred_mismatch.png]]




* The Impact of Reward Function and Epsilon on Accuracy 
Accuracy and balanced accuracy are not the main metrics that we want to optimize for, but it is helpful to report them to understand better how the model performs.

The accuracy of the agents trained using $R_{GaaFD}$ show a similar pattern as with GaaFD, as lower values of $\epsilon$ give better scores. This can be seen in figure [[fig:gaafd_best_model_accuracy]] as well as in table [[tbl:gaafd_best_models_sgd_steps]]. Figure [[fig:gaafd_best_model_accuracy]] also shows this when accounting for class imbalance through balanced accuracy. All 3 models performs perform better at classifying diastole frames compared to systole frames, as visualized in figure [[fig:gaafd_best_model_accuracy_ed_es]].


#+CAPTION: Gaussian KDE of the accuracy and balanced accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using GaaFD as the reward function. The left plot shows the accuracy. The right plot shows the balanced accuracy, which accounts more for class imbalance.
#+NAME: fig:gaafd_best_model_accuracy
[[./img/gaafd_best_model_accuracy.png]]

#+CAPTION: Gaussian KDE of the accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using GaaFD as the reward function for diastole or systole phase predictions individually. The left plot shows the accuracy for diastole frame predictions. The right plot shows the accuracy for systole frame predictions.
#+NAME: fig:gaafd_best_model_accuracy_ed_es
[[./img/gaafd_best_model_accuracy_ed_es.png]]



Again, the choice of $\epsilon$ did not matter as much for models trained using $R_{simple}$ or $R_{proximity}$ with regards to accuracy and balanced accuracy, as seen in figures [[fig:simp_best_model_accuracy]] and [[fig:prox_best_model_accuracy]]. These models also perform better at classifying diastole frames compared to systole frames.

#+CAPTION: Gaussian KDE of the accuracy and balanced accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{simple}$ as the reward function. The left plot shows the accuracy. The right plot shows the balanced accuracy, which accounts more for class imbalance.
#+NAME: fig:simp_best_model_accuracy
[[./img/simp_best_model_accuracy.png]]

#+CAPTION: Gaussian KDE of the accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{simple}$ as the reward function for diastole or systole phase predictions individually. The left plot shows the accuracy for diastole frame predictions. The right plot shows the accuracy for systole frame predictions.
#+NAME: fig:simp_best_model_accuracy_ed_es
[[./img/simp_best_model_accuracy_ed_es.png]]

#+CAPTION: Gaussian KDE of the accuracy and balanced accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{proximity}$ as the reward function. The left plot shows the accuracy. The right plot shows the balanced accuracy, which accounts more for class imbalance.
#+NAME: fig:prox_best_model_accuracy
[[./img/prox_best_model_accuracy.png]]

#+CAPTION: Gaussian KDE of the accuracy for each model ($\epsilon=0.1$, $\epsilon=0.01$, and $\epsilon=0$) when using $R_{proximity}$ as the reward function for diastole or systole phase predictions individually. The left plot shows the accuracy for diastole frame predictions. The right plot shows the accuracy for systole frame predictions.
#+NAME: fig:prox_best_model_accuracy_ed_es
[[./img/prox_best_model_accuracy_ed_es.png]]


* Learning Curves
For models trained using $R_{GaaFD}$, lower values of $\epsilon$ converge faster, as seen in the training curve in figure [[fig:gaafd_training_curves]]. There is no apparent degradation in performance over time that would indicate that the models start to overfit at some point, but the models trained using $\epsilon=0.0$ or $\epsilon=0.01$ perform slightly better on the training split than on the test split. This is not apparent for the model trained using $\epsilon=0.1$. The loss curves for all models follow a peculiar pattern where it starts with sinking rapidly before increasing again, followed by a slight decrease until it (presumably) converges, as seen in figure [[fig:gaafd_loss_curves]]. Interestingly, the model trained using a value of $\epsilon=0.01$ reaches a higher loss than the other two at its peak following the rapid sinking at the start. Also interesting, the model trained using the highest value of $\epsilon$ has a loss curve sitting between the other two.

#+CAPTION: The learning curves of using GaaFD as the reward function for different values of the exploration parameter $\epsilon$. Left: GaaFD over training time (gradient descent steps). Middle: Balanced accuracy over training time. Right: The difference in GaaFD between the validation set and the training set over training time, positive values indicating overfitting on the training set. Each point in the curve is calculated on 50 random videos in the validation (or training) set. The curves have been smoothed using a gaussian filter with a kernel standard deviation of 4 to reduce noise due to the low sample size of each data point. The overfitting (right) plot has also been smoothed using a gaussian filter with a kernel standard deviation of 50 to ensure that the overall trend is visible.
#+NAME: fig:gaafd_training_curves
[[./img/gaafd_learning_curves.png]]


#+CAPTION: The training loss over time for different values of epsilon. The left plot shows the full y-axis, while the right plot shows the same plots but with a zoomed-in y-axis.
#+NAME: fig:gaafd_loss_curves
[[./img/gaafd_loss_curves.png]]





Lower values of $\epsilon$ yields less overfitting both for models trained using $R_{simple}$ and $R_{proximity}$, as seen in figures [[fig:r1_learning_curves]] and [[fig:r2_learning_curves]]. In fact, the models are able to reach perfect accuracy on the training split, at the cost of worse performance on the test split, as seen in figure [[fig:r1r2_train_val_comparison_learning_curves]].



#+CAPTION: The training curves of using $R_{simple}$ as the reward function for different values of the exploration parameter $\epsilon$. Left: GaaFD over training time (gradient descent steps). Middle: Balanced accuracy over training time. Right: The difference in GaaFD between the validation set and the training set over training time, positive values indicating overfitting on the training set. Each point in the curve is calculated on 50 random videos in the validation (or training) set. The curves have been smoothed using a gaussian filter with a kernel standard deviation of 4 to reduce noise due to the low sample size of each data point. The overfitting (right) plot has also been smoothed using a gaussian filter with a kernel standard deviation of 50 to ensure that the overall trend is visible.
#+NAME: fig:r1_learning_curves
[[./img/r1r2_res/r1_learning_curves.png]]

#+CAPTION: The training curves of using $R_{proximity}$ as the reward function for different values of the exploration parameter $\epsilon$. Left: GaaFD over training time (gradient descent steps). Middle: Balanced accuracy over training time. Right: The difference in GaaFD between the validation set and the training set over training time, positive values indicating overfitting on the training set. Each point in the curve is calculated on 50 random videos in the validation (or training) set. The curves have been smoothed using a gaussian filter with a kernel standard deviation of 4 to reduce noise due to the low sample size of each data point. The overfitting (right) plot has also been smoothed using a gaussian filter with a kernel standard deviation of 50 to ensure that the overall trend is visible.
#+NAME: fig:r2_learning_curves
[[./img/r1r2_res/r2_learning_curves.png]]

#+CAPTION: The GaaFD over training time (gradient descent steps) on the validation set (solid pink and blue line) and the training set (dashed pink and blue lines). The GaaFD on the training set reaches 0, meaning perfect predictions.
#+NAME: fig:r1r2_train_val_comparison_learning_curves
[[./img/r1r2_res/r1r2_train_val_comparison_learning_curves.png]]



The loss curves for models trained using $R_{simple}$ or $R_{proximity}$ do not show the same peculiar pattern as those trained using $R_{GaaFD}$. They all decrease quite rapidly at first before slowing down until convergence. The model trained with the lowest value of $\epsilon$ ($\epsilon=0.1$) converges the fastest. The models trained using $R_{proximity}$ also converge faster overall.

#+CAPTION: The training loss over time for different values of epsilon. Left: an agent trained using $R_{simple}$. Right: an agent trained using $R_{proximity}$.
#+NAME: fig:r1r2_loss
[[./img/r1r2_res/r1r2_loss.png]]



Models trained using $R_{proximity}$ consistently perform better at the metric of GaaFD in later iterations of training. However, this is most apparent long after the models have already overfitted on the training split. The models trained using $R_{simple}$ have indications of performing better at the metric of balanced accuracy, though the difference is most apparent in the models trained using a value of $\epsilon=0.1$. Even though $R_{proximity}$ performs better at the important metric of GaaFD after overfitting occurs, the best model overall belongs to the model trained using $R_{simple}$ with a value of $\epsilon=0.5$.


#+CAPTION: Comparison of the training curves using $R_{simple}$ versus $R_{proximity}$ for different values of of the exploration parameter $\epsilon$. The top row shows the GaaFD over training time (gradient descent steps). The bottom row shows the balanced accuracy over training time. Each column correspond to one of the agents, $\epsilon=0.1$, $\epsilon=0.5$, and $\epsilon=1.0$, respectively.
#+NAME: fig:r1r2_comparison_learning_curves
[[./img/r1r2_res/r1r2_comparison_learning_curves.png]]







* The Impact of Reward Function and Epsilon on Q-Values
For DQN, the Q-values dictate what actions are taken. This section plots the Q-values for each reward function's best and worst-performing videos and the value of epsilon used for the reward function. These results are mainly qualitative but shed light on how the models "reason" about the frames in a video.

Figures [[fig:gaafd_best_videos_q]], [[fig:r1_q_best_videos]], and [[fig:r2_q_best_videos]] plot the Q-values of each frame in the 3 best performing videos for that model for models trained with $R_{GaaFD}$, $R_{simple}$, and $R_{proximity}$, respectively, and for each value of $\epsilon$. Likewise, Figures [[fig:gaafd_best_videos_q]], [[fig:r1_q_best_videos]], and [[fig:r1_q_best_videos]] plots the same, but for the 3 worst performing videos.

The effect of a higher value of $\epsilon$ of agents trained with $R_{GaaFD}$ is that the values of marking a frame as diastole or as systole grows closer, as seen in figure [[fig:gaafd_best_videos_q]]. There is also a noticeable positive spike for systole values in the middle of the diastole phase, mostly visible for lower values of $\epsilon$.

The effect of using $R_{simple}$ over $R_{proximity}$ seems to be that $R_{simple}$ causes less noisy Q-values. Interestingly, the spikes in the middle of systole are also visible for agents trained with $R_{simple}$ and $R_{proximity}$, though slightly less than for those trained with $R_{GaaFD}$.


#+CAPTION: The Q-values for three of the best-predicted videos for each model trained using $R_{GaaFD}$. The top row is the model with $\epsilon=0$, the middle row is the model with $\epsilon=0.01$, and the bottom row is the model with $\epsilon=0.1$. The x-axis represents time in the video. 
#+NAME: fig:gaafd_best_videos_q
[[./img/gaafd_best_videos_q.png]]

#+CAPTION: The Q-values for three of the best-predicted videos for each model trained using $R_{simple}$. The top row is the model with $\epsilon=0.1$, the middle row is the model with $\epsilon=0.5$, and the bottom row is the model with $\epsilon=1.0$. The x-axis represents time in the video. 
#+NAME: fig:r1_q_best_videos
[[./img/r1r2_res/r1_q_best_videos.png]]

#+CAPTION: The Q-values for three of the best-predicted videos for each model trained using $R_{proximity}$. The top row is the model with $\epsilon=0.1$, the middle row is the model with $\epsilon=0.5$, and the bottom row is the model with $\epsilon=1.0$. The x-axis represents time in the video. 
#+NAME: fig:r2_q_best_videos
[[./img/r1r2_res/r2_q_best_videos.png]]





#+CAPTION: The Q-values for three of the worst predicted videos for each model trained using $R_{GaaFD}$. The top row is the model with $\epsilon=0$, the middle row is the model with $\epsilon=0.01$, and the bottom row is the model with $\epsilon=0.1$. The x-axis represents time in the video.
#+NAME: fig:gaafd_worst_videos_q
[[./img/gaafd_worst_videos_q.png]]

#+CAPTION: The Q-values for three of the worst predicted videos for each model trained using $R_{simple}$. The top row is the model with $\epsilon=0.1$, the middle row is the model with $\epsilon=0.5$, and the bottom row is the model with $\epsilon=1.0$. The x-axis represents time in the video. 
#+NAME: fig:r1_q_worst_videos
[[./img/r1r2_res/r1_q_worst_videos.png]]

#+CAPTION: The Q-values for three of the worst predicted videos for each model trained using $R_{proximity}$. The top row is the model with $\epsilon=0.1$, the middle row is the model with $\epsilon=0.5$, and the bottom row is the model with $\epsilon=1.0$. The x-axis represents time in the video. 
#+NAME: fig:r2_q_wrong_videos
[[./img/r1r2_res/r2_q_wrong_videos.png]]



* Inference speed
The inference time is faster on the CPU than the GPU when we include IO roundtrip time. However, ignoring IO, the network performs extremely fast on the GPU; processing a batch of 128 frames takes just a little over a millisecond, producing a framerate of over $125\,000$ FPS. However, this is merely considered a "fun fact." A realistic scenario would include IO roundtrip time. A single frame may be processed on the CPU in 0.80 milliseconds, meaning that it can likely be included as a step in a processing stream.


#+CAPTION: The compilation time and average elapsed time over 1000 calls for the neural network, on the CPU and the GPU, with or without IO overhead.
#+NAME: tbl:simple_net_inference_speed
+-----------------------+--------------+------------------+------------------+
| Device                | # frames     | Compilation time | Average run-time |
+-----------------------+--------------+------------------+------------------+
| CPU                   | 128 frames   | 273.95 ms        | 29.15 ms         |
|                       | Single frame | 205.93 ms        | 0.80 ms          |
+-----------------------+--------------+------------------+------------------+
| GPU (including IO)    | 128 frames   | 2399.56 ms       | 34.43 ms         |
|                       | Single frame | 418.77 ms        | 2.88 ms          |
+-----------------------+--------------+------------------+------------------+
| GPU (pre-placed data) | 128 frames   | 251.10 ms        | 1.02 ms          |
|                       | Single frame | 285.56 ms        | 0.17 ms          |
+-----------------------+--------------+------------------+------------------+



* M-Mode Binary Classification Environment
A single experiment was run using the m-mode binary classification environment (MMBCE), the result of which can be seen in table [[tbl:mmode_best_model_metrics]]. The percentage of episodes where the agent actively explores its environment by moving the synthetic m-mode line is reported in addition to the key metrics. The agent is said to have explored if at least one of the actions in the episode moved or rotated the line. The GaaFD is also reported for episodes where the agent performs some exploration and where it performs no exploration individually.

The agent trained on the MMBCE performs worse than any agent trained on the BCE, as seen in table [[tbl:mmode_best_model_metrics]]. It also performs very little exploration of the environment, where almost 35% of episodes contain no movement of the synthetic m-mode line at all. Figure [[fig:action_frequencies_mmode]] show the distribution of actions taken by the agent on the test split. Over 90% of actions were of marking the current frame as either diastole or systole.

Furthermore, in the episodes where the agent /did/ perform any exploration, the agent performed worse than in the ones it did not move the synthetic m-mode line at all, as visualized in figure [[fig:mmode_gaafd_explore_or_not]]. Table [[tbl:mmode_best_model_metrics]] reports that the agent had an average GaaFD score of $5.47$ for episodes where it performed exploration versus $3.13$ for episodes where the line was still.


#+CAPTION: Performance of agents trained on the m-mode binary classification environment.
#+NAME: tbl:mmode_best_model_metrics
| Best model SGD step                    | $23\,080$ |
| GaaFD                                  | $4.66$    |
| GaaFD ED                               | $4.85$    |
| GaaFD ES                               | $4.37$    |
| % episodes with exploration            | $65.22\%$ |
| % episodes without exploration         | $34.78\%$ |
| GaaFD for episodes with exploration    | $5.47$    |
| GaaFD for episodes without exploration | $3.13$    |
| % valid aaFD                           | $59.29\%$ |
| aaFD                                   | $2.22$    |


#+CAPTION: A bar chart showcasing the distribution of actions selected by the agent. The vast majority of actions are that of marking frames as diastole or systole. To the left are all actions, while to the right are only movement actions, i.e., marking a frame as diastole or systole not included.
#+NAME: fig:action_frequencies_mmode
[[./img/action_frequencies_mmode.png]]


#+CAPTION: A density plot of GaaFD for episodes where the agent performed no other actions than marking frames as diastole or systole, i.e., no exploration, versus the density plot of GaaFD for episodes where the agent moved the synthetic m-mode line in any way at least once.
#+NAME: fig:mmode_gaafd_explore_or_not
[[./img/mmode_gaafd_explore_or_not.png]]


Moreover, the MMBCE agent is significantly slower at inference than the BCE agents, taking multiple seconds to evaluate a full video of 128 frames. It also takes more than three times as long when running the agent on the GPU.

#+CAPTION: The average compilation and run time for predicting the phase of 128 frames in a video (including IO overhead).
#+NAME: tbl:mmode_inference
+--------+------------------+--------------------------------+
| Device | Compilation time | Average run time for 128 steps |
+--------+------------------+--------------------------------+
| CPU    | 389.70 ms        | 3169.83 ms                     |
+--------+------------------+--------------------------------+
| GPU    | 2384.04 ms       | 9738.50 ms                     |
+--------+------------------+--------------------------------+












\chapter{Discussion}
* Generalized Average Absolute Frame Difference Reward Function
As seen in table [[tbl:gaafd_best_models_sgd_steps]], the best value of $\epsilon$ when using $R_{GaaFD}$ was $0.0$. This is likely because the learner has access to noisier signals the higher the value of $\epsilon$. Recall that the agent only receives a reward at the very end of the episode, which on average lasts for 50 steps. Any mistake in those 50 steps will be penalized, and the agent has no way of knowing whether it was penalized for an action taken under its policy or an action taken randomly.

Further evidence of this can be found in the loss curves in figure [[fig:gaafd_loss_curves]], as generally, the models with $\epsilon\in\{0.01, 0.1\}$ have a greater loss at the end of training. A greater loss indicates that the model is more "surprised" by the data, which could be explained by the fact that when it makes a mistake through random exploration, the model will not know which action in the episode was the true culprit.

Another interesting feature of the loss curves is the valleys at the beginning of training. At the beginning of training, the model has no knowledge about the data, and any prediction will be random. As the actors learn which action to pick, the sample data distribution changes, reflecting the new policies. This in turn creates a change in loss as the learner "catches up" to the new policy. As the model approaches a reasonable estimate of the true Q-value $Q^*$, it will make fewer mistakes, and the loss will decrease.












* On Reporting Average Absolute Frame Difference 
For all three reward functions, the percentage of videos on which we may calculate aaFD is relatively low, with the best agent predicting the correct number of events in just 80.26% of the videos. This must be considered when comparing the result against related work that has not reported this problem in their results.

For agents trained using $R_{GaaFD}$, we see a relative increase in videos that have a difference in the number of predicted and ground truth events equal to 2. This may be because the model sometimes predicts rogue frames with wrong labels, perhaps due to noise, which are quickly fixed in the following frames. This creates two events in rapid succession, as visualized in figure [[ed_es_from_predictions_2]]. Post-processing the predictions and removing noise will likely decrease the mismatch between the number of predicted and ground truth events.

#+CAPTION: A single wrongly predicted phase that is corrected right after creates two incorrect events.
#+NAME: ed_es_from_predictions_2
[[./img/ed_es_from_predictions.png]]




* Q-Values



Peeking inside the machinery of the DQN-agent, we see a potential cause of the performance discrepancy between the three models. For every frame, the agent predicts the future returns of marking a frame as diastole or systole. These predictions are plotted in figures [[fig:gaafd_best_videos_q]] and [[fig:gaafd_worst_videos_q]]. The model that uses $\epsilon=0$ better differentiates the value of taking either action for a given state. 




TODO: The rest
















* Why Use Reinforcement Learning?
In the experiments that use the reward function $R_{GaaFD}$, we have seen that the RL agent can learn from a very sparse reward signal. This makes RL a very general tool that can be used when supervised learning methods are not applicable. However, there is no such thing as a free lunch, and bringing in the whole RL machinery for a classification task brings much complexity.

The methods that have been used in this thesis suffer from low data efficiency compared to a supervised learning approach. Each sample given to the learner is of only one of the phases, as only one phase is predicted at every step. We also applied additional data sampling constraints on the learner by enforcing that a data item should be sampled 0.5 on average, i.e., half of the data is discarded. For the formulations where the agent's actions do not affect future actions, such as in BCE with either phase classification reward function $R_{simple}$ and $R_{proximity}$, the sampling should arguably be at least 1. The value of 0.5 for these experiments was due to running the experiments with $R_{GaaFD}$ first, and it was overlooked in subsequent experiments.

The environment abstraction for sampling was also a significant performance bottleneck for the BCE environment. Every new sample had to be created by stepping through the environment on the CPU. For the BCU environment, whose next states were completely independent of the action taken by the actors, this abstraction limited us from batching the inference on the GPU. Because the environment abstraction is such an essential part of the RL ecosystem and libraries, we still opted to use it.

A synthetic m-mode version of BCE was included to give RL a fair shot at proving its usefulness. The search for a synthetic m-mode line to base future decisions on would be very challenging to solve using supervised learning, and here RL is assumed to be the best tool for the job. However, this was a tricky problem, presumably too complex for the current setup, and the benefits are not all apparent. Even if it did work, its inference would have been much slower than that of BCE since there would be no way of batching forward passes through the neural network.

However, even though RL may not be the best tool for phase detection, it is still a promising technology, especially for problems that require exploration.



* Lack of Comparison Experiments
Only one other study was found to report their model's performance on the Echonet dataset \cite{lane_multibeat_2021}. This is considered a weakness of this project, and getting access to multiple datasets early in the project should have been a priority. This makes it hard to gain precise insight into the performance of the methods versus supervised learning methods.

The authors report an average aaFD of $2.30$ and $3.49$ for ED and ES events in their paper. Our best model can report an average aaFD of $1.69$ overall, but this is only for 80% of the videos, as 20% have an incorrect number of predicted events compared to ground truth events. The authors do not report how many, if any, of the videos had an incorrect number of predicted events.








\chapter{Conclusion and Further Work}
We are now ready to answer the original research questions:

*Is it possible to use reinforcement learning for the task of ED-/ES-frame detection?* Yes, there are multiple ways, but some are more efficient than others. RL is very flexible and allows us to model the problem in many different ways, both in terms of reward functions and environment dynamics.

*How does the formulation of the problem as a reinforcement learning problem affect the performance of the model?* 
We have seen that the design of the reward function matters a lot. If the reward signal is too sparse, it will be more difficult for the agent to learn, resulting in poorer performance, as seen in our experiments. Our test results comparing the reward function $R_{simple}$ and $R_{proximity}$ shows that $R_{simple}$ performs better at aaFD, even though $R_{proximity}$ was specifically designed to be more similar to aaFD. This is despite the learning curves seen in figure [[fig:r1r2_comparison_learning_curves]] indicating that $R_{proximity}$ performs better at GaaFD after the models have begun to overfit because $R_{simple}$ can reach a better score just before overfitting occurs.

Furthermore, the value of $\epsilon$ has been a crucial hyperparameter for these tasks. For the sparse GaaFD reward function, using a value of $\epsilon=0$ yielded the best results. For $R_{simple}$ and $R_{proximity}$, the difference between different values of $\epsilon$ had little effect on the performance of the best model but seemed to have a regularizing effect. It would be interesting to dive deeper into why this happens and perhaps how we can take advantage of it when sampling data for supervised learning.





\backmatter{}

\printbibliography{}
