* Introduction
A simple formulation was chosen to act as a baseline. The environment is an ultrasound video of the heart with labels on which cardiac phase each frame is in. For every step, the agent is shown the current frame plus the three previous and the three next frames, and makes a prediction on which cardiac phase the current frame is in. The agent thus have two possible actions: mark the current frame as Diastole or mark it as Systole. After each prediction, the current frame is moved one frame forwards, and the episode ends once the current frame is the last frame of the video.

#+CAPTION: Visualization of the Binary Classification Environment loop. An agent sees the observation from the current frame and takes an action, either marking it as Diastole or as Systole, and gets back the reward and the observation for the next frame from the environment.
#+NAME: fig:bce_loop
[[./img/binary_classification_environment_loop.png]]

The agent doesn't directly predict which frames are ED and which frames are ES, but after an agent has made predictions for every frame we can easily find them by looking at the transitions from predicted Diastolic to predicted Systolic.

#+CAPTION: How to find the End-Diastolic (ED) and End-Systolic (ES) from predicted phase. Upper section: good predictions. Lower section: noisy predictions. The noisier the predictions are, the more ED and ES events are incorrectly predicted.
#+NAME: fig:bce_predicted_events
[[./img/ed_es_from_predictions.png]]

Because the job of the agent is to classify each frame as either Diastole or Systole, we name this formulation "Binary Classification Environment".


* TODO Reward functions
The reward function is one of the most important things to consider when designing a RL formulation. It is what guides the agent and must therefore represent our goals for the algorithm. 

The standard for measuring performance in the ED-/​ES-frame detection task is the /average absolute frame difference (aaFD)/. aaFD is defined as
\[aaFD=\frac{1}{N}\sum_{t=1}^N|y_t-\hat{y}_t|\]
where N is the number of ED/ES events, $y_t$ is the ground truth event at time $t$ (of $N$), and $\hat{y}_t$ is the predicted event at time $t$.

aaFD is the key metric that we want to optimize (minimize), and should therefore be considered for the reward function. There are a couple of problems with using aaFD as a reward signal, however: 
1. aaFD is calculated on a whole sequence of frames, meaning that we can't give the agent a reward until it has made a prediction for each frame. This happens at the end of the episode, which is usually tens of frames long in the training dataset. Giving a reward too infrequently makes the agent harder to train and is called the /sparse reward problem/ [TODO: citation?].
2. aaFD is not well defined for when there is an unequal number of predicted events as there are ground truth events, which there invariably will be during training (see lower section of figure [[fig:bce_predicted_events]]).

We could instead continue to treat it as a binary classification problem and give positive rewards for correct predictions, and negative rewards for incorrect ones. Then we would be able to give a reward after every step, making the reward signal no longer sparse.

We must still decide how much reward will be given for correct predictions and how much penalty will be given for incorrect ones. A simple reward function is to give a reward of 1 for correct ones and 0 for incorrect ones:

\begin{equation}
  R_1(s, a) \triangleq
    \left\{
	    \begin{array}{ll}
		    1 & \mbox{if } s=a \\
  	  	0 & \mbox{if } s\neq a
	    \end{array}
    \right
\end{equation}

We could also do the same, but penalize incorrect predictions more:

\begin{equation}
  R_2(s, a) \triangleq
    \left\{
	    \begin{array}{ll}
		    1 & \mbox{if } s=a \\
  	  	-1 & \mbox{if } s\neq a
	    \end{array}
    \right
\end{equation}

We could also penalize incorrect predictions according to how wrong they were:

\begin{equation}
  R_3(s, a) \triangleq 1-d(s, a)
\end{equation}

where $\d(s,a)$ is the number of frames to the closest frame with the predicted phase. This gives a reward of 1 for correct predictions, 0 for a one-off incorrect prediction, and a penalty proportional to how off it was.


TODO: plot reward functions together.




TODO: Note to self:
- Things to try out:
  - Defining a robust aaFD
  - Try to use it as a reward signal at the end of an episode (or balanced accuracy if can't make robust aaFD).
  - Give higher penalties for incorrect predictions that are close to transitions instead of the opposite (as in distance-based).




* TODO Similarities to SOTA, regression
Earlier work on detecting ED and ES frames have used a supervised learning approach, formulating the problem as a regression task[TODO: citations from essay]. The line being regressed encodes the phase such that is increasing while in Diastolic phase and decreasing while in Systolic phase. This is in contrast to simply treating the problem as a classification task, using the classes "ED", "ES", and "Trivial", which would result in a class imbalance because the "Trivial" class is greatly over-represented.

Our Binary Classification Environment also gets around the imbalance problem by predicting only the current phase, either Diastole or Systole. There is still a natural class imbalance however, in that there generally are more Diastole frames than Systole, but it is now in a more reasonable range.

We selected a simple DQN[TODO: citation] agent to train as a baseline. DQN uses Q-learning, which uses a state-action value function for selecting actions. In practice, the state-action value function is a neural network that takes the current state as input and outputs the predicted value of taking each action. A greedy policy may then select the action that has the highest predicted value. Because the agent doesn't affect the environment in any way by choosing one action over another, we could just as well have trained the Q-value function directly in a supervised learning manner. This means that under the hood, the agent is really just solving a regression problem, though with a moving ground truth. The function being fitted is two lines representing the value of taking the Diastole action and the Systole action. The shape of the lines depend on the reward function and the discount factor.

TODO: Try to visualize how the regression line looks for the two reward schemes?

For this reason, we don't think that the Binary Classification Environment formulation will perform better than the curent state-of-the-art supervised learning approaches, unless there is some fundamental advantage to regressing on two lines instead of one. It is however a good baseline that we can build upon.


* TODO Evaluation metrics
The standard for measuring performance in the ED-/​ES-frame detection task is the /average absolute frame difference (aaFD)/. aaFD is defined as
\[aaFD=\frac{1}{N}\sum_{t=1}^N|y_t-\hat{y}_t|\]
where N is the number of ED/ES events, $y_t$ is the ground truth event at time $t$ (of $N$), and $\hat{y}_t$ is the predicted event at time $t$.

But as can be seen in figure [[fig:bce_predicted_events]], when the predictions are noisy and imperfect there will be more predicted events than there are in the ground truth. The above definition of aaFD requires there to be an equal number of predicted events as there are in the ground truth. But since the binary classification environment formulation is so similar to just a regular classification problem it lends itself to classification metrics, such as accuracy, precision, recall etc.



* The network, hyper-parameters, algorithm, etc.
Our network is based on the original Atari DQN network. It consists of a convolutional layer with 16 8x8 kernels and a stride of 4, followed by a ReLU activation layer, then another convolutional layer with 32 4x4 kernels and a stride of 2, also followed by a ReLU activation layer. This is then flattened and passed through a fully connected layer with 256 outputs, which is then passed through yet another ReLU layer, before finally being passed through a fully connected with two outputs representing the value of each of the two possible actions.

The agent uses a discount factor of 0.99 and 5-step bootstrapping.



* TODO Choice of frameworks
- JAX
  - Haiku
  - Optax
- Acme
  - Rlax
  - Distributed algorithms with Launchpad
  - Reverb

* TODO Performance insight app
- Redo in browser with ClojureScript
- Log metrics for each video over time and show in PerfIn


* TODO Describe dataset (Echonet)




